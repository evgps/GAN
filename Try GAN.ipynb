{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Generative Adversarial Networks (GAN) example in PyTorch.\n",
    "# See related blog post at https://medium.com/@devnag/generative-adversarial-networks-gans-in-50-lines-of-code-pytorch-e81b79659e3f#.sch4xgsa9\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"6\" # change 0  with whatever card is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data params\n",
    "data_mean = 4\n",
    "data_stddev = 1.25\n",
    "\n",
    "# Model params\n",
    "g_input_size = 1     # Random noise dimension coming into generator, per output vector\n",
    "g_hidden_size = 50   # Generator complexity\n",
    "g_output_size = 1    # size of generated output state vector\n",
    "\n",
    "m_input_size = 1   # State dimension coming into model, per output vector\n",
    "m_hidden_size = 50   # Model complexity\n",
    "m_output_size = 1    # size of generated output vector\n",
    "\n",
    "minibatch_size = 200\n",
    "\n",
    "g_learning_rate = 2e-4  # 2e-4\n",
    "m_learning_rate = 2e-4  # 2e-4\n",
    "\n",
    "optim_betas = (0.9, 0.999)\n",
    "num_epochs = 50000\n",
    "print_interval = 200\n",
    "m_steps = 1  # 'k' steps in the original GAN paper. Can put the discriminator on higher training freq than generator\n",
    "g_steps = 1\n",
    "\n",
    "# # ### Uncomment only one of these\n",
    "# #(name, preprocess, d_input_func) = (\"Raw data\", lambda data: data, lambda x: x)\n",
    "# (name, preprocess, d_input_func) = (\"Data and variances\", lambda data: decorate_with_diffs(data, 2.0), lambda x: x * 2)\n",
    "\n",
    "# print(\"Using data [%s]\" % (name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1(x, y):\n",
    "    loss = (x-y).pow(2).sum()\n",
    "    return loss\n",
    "\n",
    "def nl1(x, y):\n",
    "    loss = -((x-y).pow(2).sum())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### DATA: Target data and generator input data\n",
    "def exact_model(mu, sigma):\n",
    "    return lambda x: torch.exp(-((x-mu)**2)/sigma**2)/np.sqrt(6.28)/sigma  # Gaussian\n",
    "\n",
    "def get_generator_input_sampler():\n",
    "    return lambda m, n: torch.rand(m, n)  # Uniform-dist data into generator, _NOT_ Gaussian\n",
    "\n",
    "# ##### MODELS: Generator model and discriminator model\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.map1(x)\n",
    "        x = F.elu(self.map2(x))\n",
    "        return self.map3(x)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.map1 = nn.Linear(input_size, hidden_size)\n",
    "        self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.elu(self.map1(x))\n",
    "        x = F.elu(self.map2(x))\n",
    "        \n",
    "        return self.map3(x)\n",
    "\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, output_size):\n",
    "#         super(Discriminator, self).__init__()\n",
    "#         self.map1 = nn.Linear(input_size, hidden_size)\n",
    "#         self.map2 = nn.Linear(hidden_size, hidden_size)\n",
    "#         self.map3 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.elu(self.map1(x))\n",
    "#         x = F.elu(self.map2(x))\n",
    "#         return F.sigmoid(self.map3(x))\n",
    "\n",
    "def extract(v):\n",
    "    return v.data.storage().tolist()\n",
    "\n",
    "\n",
    "def stats(d):\n",
    "    return [np.mean(d), np.std(d)]\n",
    "\n",
    "def decorate_with_diffs(data, exponent):\n",
    "    mean = torch.mean(data.data, 1, keepdim=True)\n",
    "    mean_broadcast = torch.mul(torch.ones(data.size()), mean.tolist()[0][0])\n",
    "    diffs = torch.pow(data - Variable(mean_broadcast), exponent)\n",
    "    return torch.cat([data, diffs], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 L1: G:Variable containing:\n",
      "-0.5067\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 0.5067\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      " 3.2136e-02\n",
      " 3.4872e-02\n",
      " 8.1771e-02\n",
      " 6.4766e-03\n",
      " 2.8789e-02\n",
      " 2.8675e-02\n",
      " 9.8288e-02\n",
      "-8.3785e-03\n",
      " 6.0016e-03\n",
      " 1.6382e-02\n",
      "-2.3044e-03\n",
      " 8.6832e-02\n",
      "-2.1289e-03\n",
      " 7.0945e-02\n",
      "-5.7542e-03\n",
      "-3.4859e-04\n",
      " 4.5356e-02\n",
      " 8.4326e-02\n",
      " 1.0606e-02\n",
      "-3.9366e-03\n",
      " 5.0774e-02\n",
      " 3.4312e-02\n",
      " 4.9893e-02\n",
      " 1.0243e-01\n",
      "-4.0676e-03\n",
      " 1.0016e-01\n",
      " 3.9299e-02\n",
      " 4.7989e-02\n",
      " 8.6637e-02\n",
      " 8.8312e-02\n",
      " 1.9611e-02\n",
      " 5.6707e-02\n",
      " 1.6018e-03\n",
      " 4.6439e-02\n",
      " 2.4045e-02\n",
      " 7.5301e-02\n",
      " 1.6362e-02\n",
      " 9.0811e-03\n",
      " 3.7990e-02\n",
      " 1.9693e-02\n",
      "-6.4602e-03\n",
      " 4.1787e-02\n",
      " 5.4031e-02\n",
      " 7.4609e-02\n",
      " 7.9838e-02\n",
      "-4.2650e-03\n",
      " 5.3019e-02\n",
      " 2.7853e-02\n",
      " 2.1519e-02\n",
      " 1.1428e-02\n",
      "-1.0780e-03\n",
      " 5.3239e-02\n",
      " 8.0709e-02\n",
      " 2.6485e-02\n",
      " 2.6820e-02\n",
      " 7.8316e-02\n",
      " 3.0489e-02\n",
      " 4.7395e-02\n",
      " 2.3149e-02\n",
      " 9.5153e-02\n",
      " 4.5277e-02\n",
      " 8.6480e-02\n",
      " 2.3381e-02\n",
      " 3.7431e-02\n",
      "-6.7275e-03\n",
      " 4.7602e-03\n",
      " 9.2864e-02\n",
      "-5.7677e-03\n",
      " 6.5141e-02\n",
      " 3.2125e-02\n",
      "-1.8179e-03\n",
      " 7.0335e-02\n",
      " 4.9249e-02\n",
      " 8.9975e-02\n",
      " 9.9661e-02\n",
      " 3.9247e-02\n",
      "-8.3052e-03\n",
      " 6.9443e-02\n",
      " 1.7085e-02\n",
      " 3.0506e-02\n",
      "-4.5089e-03\n",
      " 6.2653e-02\n",
      " 1.2292e-04\n",
      " 8.3355e-02\n",
      " 1.0139e-01\n",
      " 5.4140e-02\n",
      " 3.7177e-02\n",
      " 3.0690e-02\n",
      " 8.2760e-02\n",
      "-7.8148e-03\n",
      " 9.4704e-02\n",
      " 2.5556e-02\n",
      " 2.4704e-02\n",
      " 8.3166e-02\n",
      " 1.2418e-02\n",
      " 7.0510e-02\n",
      " 5.4179e-03\n",
      "-2.2224e-03\n",
      " 9.0712e-02\n",
      " 4.7864e-02\n",
      " 4.8294e-02\n",
      "-4.3604e-04\n",
      " 6.8249e-02\n",
      " 9.7539e-02\n",
      " 3.3003e-02\n",
      " 3.0020e-03\n",
      " 7.8498e-02\n",
      " 9.3165e-02\n",
      " 1.9924e-02\n",
      " 7.8814e-02\n",
      " 5.0268e-02\n",
      " 7.8869e-02\n",
      " 8.2575e-02\n",
      " 3.0527e-02\n",
      " 7.5653e-02\n",
      " 1.0710e-02\n",
      " 9.3694e-02\n",
      " 4.4479e-02\n",
      " 4.9436e-02\n",
      " 7.9444e-02\n",
      " 4.6313e-02\n",
      "-5.9643e-03\n",
      " 8.9142e-02\n",
      " 8.0320e-02\n",
      " 3.0200e-02\n",
      " 2.6599e-02\n",
      " 7.4872e-02\n",
      " 9.5406e-02\n",
      " 1.5924e-02\n",
      " 3.0655e-02\n",
      " 8.7422e-02\n",
      "-8.0673e-03\n",
      " 6.7357e-02\n",
      " 4.5015e-02\n",
      " 2.4915e-02\n",
      " 8.6361e-02\n",
      " 7.8698e-02\n",
      " 6.8066e-02\n",
      " 6.7498e-02\n",
      " 7.9282e-02\n",
      " 7.9475e-02\n",
      " 1.1955e-02\n",
      "-5.1893e-03\n",
      " 1.9189e-02\n",
      " 2.7407e-02\n",
      " 7.6103e-03\n",
      " 6.2770e-02\n",
      " 9.7747e-02\n",
      " 3.0453e-02\n",
      " 6.7204e-02\n",
      " 9.8626e-02\n",
      "-3.0062e-03\n",
      " 1.8031e-02\n",
      " 5.7876e-02\n",
      " 6.3623e-02\n",
      " 6.2461e-02\n",
      " 7.9343e-02\n",
      " 1.8612e-02\n",
      " 4.9867e-02\n",
      " 5.5834e-02\n",
      " 5.3868e-06\n",
      " 1.3129e-03\n",
      " 8.4720e-02\n",
      " 3.1664e-02\n",
      " 2.6322e-02\n",
      "-1.2668e-03\n",
      "-3.8370e-03\n",
      " 4.1632e-02\n",
      " 6.2640e-02\n",
      " 5.3585e-02\n",
      " 1.3153e-02\n",
      " 4.2114e-02\n",
      " 1.4217e-03\n",
      " 6.3631e-02\n",
      " 5.3867e-02\n",
      " 7.1512e-02\n",
      " 6.0106e-02\n",
      " 7.3497e-02\n",
      "-4.8991e-03\n",
      " 4.3616e-02\n",
      "-1.9468e-03\n",
      " 5.7154e-03\n",
      " 7.2097e-02\n",
      " 6.0944e-03\n",
      " 8.9034e-02\n",
      "-4.1887e-03\n",
      " 7.4670e-02\n",
      "-4.6229e-04\n",
      " 7.8653e-02\n",
      " 2.1394e-03\n",
      " 2.9631e-02\n",
      " 9.7788e-02\n",
      " 8.2633e-03\n",
      "-4.4460e-03\n",
      " 4.5149e-03\n",
      " 8.2041e-02\n",
      " 5.8181e-03\n",
      " 7.3641e-02\n",
      " 3.8096e-02\n",
      " 1.6794e-02\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [1.4363191357915639e-05, 2.4454173066574998e-06], Fake: [-0.049844402335584161, 0.0069052121988953637]) \n",
      "200 L1: G:Variable containing:\n",
      "-9.3183e+06\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 9.3183e+06\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "-2138.5461\n",
      "-1390.8185\n",
      "-1348.2852\n",
      "-1173.6541\n",
      "-2008.6410\n",
      "-2178.6250\n",
      "-2208.5652\n",
      "-1612.1431\n",
      "-1849.8715\n",
      "-2232.0703\n",
      "-1690.6957\n",
      "-2037.2983\n",
      "-1950.5350\n",
      "-2060.2329\n",
      "-1867.5306\n",
      "-1422.4617\n",
      "-1403.2778\n",
      "-1517.9154\n",
      "-1487.4257\n",
      "-1616.6311\n",
      "-1674.6448\n",
      "-1849.2469\n",
      "-1330.8782\n",
      "-1832.0060\n",
      "-1944.2240\n",
      "-1228.1187\n",
      "-1442.0587\n",
      "-1903.0293\n",
      "-2093.6509\n",
      "-1927.9930\n",
      "-2092.5598\n",
      "-1299.4583\n",
      "-1866.7582\n",
      "-1661.2340\n",
      "-1715.9656\n",
      "-1567.4084\n",
      "-1516.0453\n",
      "-1790.6394\n",
      "-1533.3698\n",
      "-1242.8728\n",
      "-2170.4741\n",
      "-1899.5598\n",
      "-2090.9543\n",
      "-1335.7579\n",
      "-1751.2874\n",
      "-1798.6289\n",
      "-1370.2782\n",
      "-1374.0400\n",
      "-1230.5779\n",
      "-1505.1591\n",
      "-2114.6289\n",
      "-2187.5608\n",
      "-1287.3677\n",
      "-1758.4595\n",
      "-1594.5863\n",
      "-1388.3184\n",
      "-2200.0161\n",
      "-1438.1758\n",
      "-1362.4363\n",
      "-1387.3596\n",
      "-1800.7992\n",
      "-1712.1266\n",
      "-1160.5798\n",
      "-1414.4049\n",
      "-1925.2791\n",
      "-1174.9514\n",
      "-1553.1199\n",
      "-2138.0137\n",
      "-1792.1168\n",
      "-1170.6416\n",
      "-1775.9670\n",
      "-1215.3630\n",
      "-2236.3726\n",
      "-1586.5394\n",
      "-1415.8094\n",
      "-1624.6239\n",
      "-1385.0399\n",
      "-1268.8789\n",
      "-2188.9414\n",
      "-1969.8279\n",
      "-1171.9956\n",
      "-1585.0341\n",
      "-1899.5647\n",
      "-1509.2548\n",
      "-2198.1858\n",
      "-1987.9927\n",
      "-1928.1488\n",
      "-1378.3271\n",
      "-2119.1472\n",
      "-1289.0984\n",
      "-1699.2892\n",
      "-1535.3312\n",
      "-1284.2671\n",
      "-1853.0143\n",
      "-1932.9670\n",
      "-2100.6797\n",
      "-1545.2362\n",
      "-1636.5990\n",
      "-1222.0498\n",
      "-1404.3973\n",
      "-1168.6381\n",
      "-2054.1597\n",
      "-1400.2532\n",
      "-1922.1908\n",
      "-2218.7412\n",
      "-1734.9457\n",
      "-2052.1763\n",
      "-1651.7709\n",
      "-1248.5979\n",
      "-2102.5486\n",
      "-1891.6514\n",
      "-1249.7969\n",
      "-2212.0957\n",
      "-1276.1989\n",
      "-1820.4369\n",
      "-1485.4768\n",
      "-1157.6949\n",
      "-1305.7227\n",
      "-2176.4915\n",
      "-2173.8098\n",
      "-2229.3914\n",
      "-1883.6493\n",
      "-1640.1146\n",
      "-2159.1292\n",
      "-1163.4318\n",
      "-2103.1592\n",
      "-1937.7483\n",
      "-1949.7814\n",
      "-2146.1904\n",
      "-1559.2600\n",
      "-1289.3340\n",
      "-1802.5619\n",
      "-1774.5721\n",
      "-2102.3547\n",
      "-1907.2013\n",
      "-1790.4556\n",
      "-1808.8473\n",
      "-1525.8658\n",
      "-1774.7084\n",
      "-1559.1367\n",
      "-1633.4034\n",
      "-1552.0175\n",
      "-1277.6200\n",
      "-2139.4077\n",
      "-1258.9688\n",
      "-1232.3164\n",
      "-1519.5977\n",
      "-1507.2117\n",
      "-1816.7072\n",
      "-2169.0276\n",
      "-1317.6422\n",
      "-1794.1503\n",
      "-1519.2168\n",
      "-1778.9735\n",
      "-1709.0828\n",
      "-1256.9631\n",
      "-1209.0702\n",
      "-1352.9935\n",
      "-1440.6887\n",
      "-2073.8135\n",
      "-1683.4471\n",
      "-1312.7189\n",
      "-2024.6107\n",
      "-1911.2393\n",
      "-1602.9750\n",
      "-1442.3574\n",
      "-1241.1637\n",
      "-1526.0875\n",
      "-1560.3844\n",
      "-1203.8246\n",
      "-1931.6646\n",
      "-2112.7947\n",
      "-1272.2300\n",
      "-2167.4653\n",
      "-1600.5148\n",
      "-2086.6243\n",
      "-1231.9727\n",
      "-1794.1064\n",
      "-1247.2911\n",
      "-1523.6486\n",
      "-1842.0918\n",
      "-1394.3434\n",
      "-1865.3265\n",
      "-1854.6205\n",
      "-2113.8191\n",
      "-1834.5714\n",
      "-1298.3169\n",
      "-1659.9124\n",
      "-1384.9100\n",
      "-1901.9828\n",
      "-1281.0117\n",
      "-1502.0225\n",
      "-2106.5288\n",
      "-1604.0619\n",
      "-1661.8003\n",
      "-1504.4166\n",
      "-1363.3878\n",
      "-1580.8811\n",
      "-1304.2067\n",
      "-1945.9971\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-212.03139854431151, 40.426174084272624]) \n",
      "400 L1: G:Variable containing:\n",
      "-1.0646e+09\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 1.0646e+09\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "-14097.0537\n",
      "-23313.1738\n",
      "-19250.9004\n",
      "-22811.6465\n",
      "-17702.4668\n",
      "-18549.7949\n",
      "-18309.2148\n",
      "-15545.8311\n",
      "-16871.1582\n",
      "-23027.2051\n",
      "-23368.5723\n",
      "-20790.8789\n",
      "-14788.9609\n",
      "-22116.4395\n",
      "-18027.8750\n",
      "-15221.7275\n",
      "-22460.4824\n",
      "-16898.5215\n",
      "-18139.9180\n",
      "-14616.1016\n",
      "-23210.1797\n",
      "-16055.4170\n",
      "-18818.7324\n",
      "-19472.4746\n",
      "-23430.6660\n",
      "-17136.3125\n",
      "-23891.8555\n",
      "-17339.7051\n",
      "-23532.4688\n",
      "-19024.2402\n",
      "-23082.7363\n",
      "-12854.2764\n",
      "-19351.3613\n",
      "-18354.5801\n",
      "-13165.6084\n",
      "-19499.0488\n",
      "-15189.5283\n",
      "-22430.0137\n",
      "-15822.3018\n",
      "-16397.6699\n",
      "-17878.7461\n",
      "-15258.4092\n",
      "-21517.2305\n",
      "-15159.2158\n",
      "-16213.1377\n",
      "-12954.2285\n",
      "-22604.5195\n",
      "-15546.8652\n",
      "-13782.2256\n",
      "-23749.3809\n",
      "-13691.1465\n",
      "-15576.1494\n",
      "-16997.5840\n",
      "-17333.8242\n",
      "-21400.6445\n",
      "-13085.4131\n",
      "-16044.8271\n",
      "-21031.9102\n",
      "-19818.1035\n",
      "-22329.4102\n",
      "-14546.7236\n",
      "-18414.7852\n",
      "-19594.3516\n",
      "-14621.5400\n",
      "-18899.0234\n",
      "-16519.3789\n",
      "-21434.6523\n",
      "-18208.8262\n",
      "-15710.7139\n",
      "-19993.3359\n",
      "-15983.8994\n",
      "-13454.2402\n",
      "-14674.0605\n",
      "-20039.4297\n",
      "-17848.1543\n",
      "-22502.8418\n",
      "-13351.7539\n",
      "-16838.3496\n",
      "-18389.3438\n",
      "-20490.8848\n",
      "-20909.2656\n",
      "-16295.3467\n",
      "-13856.5684\n",
      "-15338.3203\n",
      "-15225.9951\n",
      "-16697.0391\n",
      "-18132.5488\n",
      "-16766.9062\n",
      "-16764.2773\n",
      "-18311.3887\n",
      "-21609.4766\n",
      "-20095.4375\n",
      "-21744.2344\n",
      "-17006.1172\n",
      "-13393.9248\n",
      "-17591.4785\n",
      "-18987.1445\n",
      "-23543.2656\n",
      "-14280.0586\n",
      "-20532.5527\n",
      "-12771.1113\n",
      "-21755.5508\n",
      "-12813.2256\n",
      "-19257.6719\n",
      "-14538.6465\n",
      "-14381.7871\n",
      "-13673.1641\n",
      "-12871.9219\n",
      "-13461.5713\n",
      "-15099.1602\n",
      "-14924.8154\n",
      "-16254.1436\n",
      "-21408.9785\n",
      "-20336.1523\n",
      "-22304.4629\n",
      "-23835.6562\n",
      "-23240.8438\n",
      "-19622.7910\n",
      "-17444.8750\n",
      "-21548.5469\n",
      "-15727.4316\n",
      "-12851.6670\n",
      "-12515.7227\n",
      "-12209.2441\n",
      "-13150.0645\n",
      "-18721.7227\n",
      "-16641.8594\n",
      "-17154.7930\n",
      "-13235.5479\n",
      "-22595.0566\n",
      "-23476.1152\n",
      "-20878.6738\n",
      "-12144.0430\n",
      "-20622.9648\n",
      "-12678.5596\n",
      "-20077.4082\n",
      "-19595.1738\n",
      "-20375.1758\n",
      "-20822.3750\n",
      "-16511.5840\n",
      "-16235.4092\n",
      "-19790.8066\n",
      "-22767.5996\n",
      "-12225.4941\n",
      "-21979.1074\n",
      "-16555.2520\n",
      "-18523.4082\n",
      "-22763.0762\n",
      "-14600.8428\n",
      "-14127.9590\n",
      "-12120.9385\n",
      "-21730.7090\n",
      "-15161.6377\n",
      "-21085.5625\n",
      "-16670.1152\n",
      "-23848.7949\n",
      "-22421.9395\n",
      "-22173.4570\n",
      "-22498.0410\n",
      "-14395.7656\n",
      "-19105.4082\n",
      "-18589.5742\n",
      "-22498.8086\n",
      "-14484.9492\n",
      "-12635.0107\n",
      "-12891.1650\n",
      "-19595.2539\n",
      "-16748.7402\n",
      "-20821.3867\n",
      "-16486.8867\n",
      "-23457.5547\n",
      "-14720.1377\n",
      "-20647.8672\n",
      "-19738.8711\n",
      "-12320.2393\n",
      "-15093.8633\n",
      "-15470.5762\n",
      "-14632.4121\n",
      "-13979.5127\n",
      "-21372.1680\n",
      "-14589.7480\n",
      "-18057.9121\n",
      "-21797.4531\n",
      "-14516.4277\n",
      "-22215.2227\n",
      "-13661.5391\n",
      "-19308.6777\n",
      "-19918.8711\n",
      "-18521.2598\n",
      "-17754.7910\n",
      "-21625.2734\n",
      "-17997.0527\n",
      "-19383.5352\n",
      "-12505.7852\n",
      "-18693.7148\n",
      "-19104.7676\n",
      "-17098.3867\n",
      "-23698.9160\n",
      "-21761.6309\n",
      "-13636.5527\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-2267.0366571044924, 428.26918597528345]) \n",
      "600 L1: G:Variable containing:\n",
      "-1.5222e+10\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 1.5222e+10\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "-49102.2305\n",
      "-60804.7266\n",
      "-73630.0078\n",
      "-63286.6211\n",
      "-81391.0391\n",
      "-73000.1953\n",
      "-71670.3750\n",
      "-47568.5703\n",
      "-64785.3242\n",
      "-60466.7070\n",
      "-53001.7109\n",
      "-80847.6797\n",
      "-73733.3203\n",
      "-45455.4688\n",
      "-80171.8984\n",
      "-62529.8984\n",
      "-62640.2930\n",
      "-55351.2695\n",
      "-86435.1328\n",
      "-57632.7461\n",
      "-78929.5234\n",
      "-85499.6250\n",
      "-49165.5625\n",
      "-87112.3750\n",
      "-67590.6172\n",
      "-51932.5352\n",
      "-67212.4688\n",
      "-76577.0547\n",
      "-85338.0000\n",
      "-55937.0352\n",
      "-48472.2578\n",
      "-51248.6055\n",
      "-55368.2930\n",
      "-46393.4531\n",
      "-80680.8359\n",
      "-45435.6992\n",
      "-58989.9375\n",
      "-51677.1250\n",
      "-56204.3320\n",
      "-77603.6641\n",
      "-66758.3828\n",
      "-76778.1406\n",
      "-79629.4297\n",
      "-68230.9219\n",
      "-53452.9531\n",
      "-67339.9766\n",
      "-83536.7812\n",
      "-78260.0625\n",
      "-51776.5234\n",
      "-86296.5781\n",
      "-69770.0000\n",
      "-50058.3594\n",
      "-63038.2109\n",
      "-73553.5625\n",
      "-49700.7969\n",
      "-70663.8672\n",
      "-74036.5156\n",
      "-55239.6523\n",
      "-86908.6875\n",
      "-53689.4023\n",
      "-49746.2656\n",
      "-86070.5547\n",
      "-81708.5234\n",
      "-77358.4453\n",
      "-54089.7617\n",
      "-55792.2656\n",
      "-52027.2656\n",
      "-65835.8281\n",
      "-77641.0000\n",
      "-53081.7930\n",
      "-45607.5391\n",
      "-89193.4922\n",
      "-72627.1875\n",
      "-78657.8672\n",
      "-76051.2656\n",
      "-54767.3477\n",
      "-82623.1016\n",
      "-75742.6250\n",
      "-70914.4375\n",
      "-74199.6875\n",
      "-61691.1211\n",
      "-74223.2969\n",
      "-47787.7930\n",
      "-84935.9375\n",
      "-57581.1992\n",
      "-67203.6797\n",
      "-52504.3711\n",
      "-46842.2227\n",
      "-53195.1523\n",
      "-55361.4688\n",
      "-60957.9961\n",
      "-50558.6836\n",
      "-63205.2109\n",
      "-87181.6484\n",
      "-64799.8398\n",
      "-70532.5234\n",
      "-75692.2578\n",
      "-81145.0078\n",
      "-74599.4922\n",
      "-86118.4375\n",
      "-73136.8438\n",
      "-65242.8711\n",
      "-66490.8516\n",
      "-65176.8867\n",
      "-61753.9414\n",
      "-53082.9336\n",
      "-76824.1719\n",
      "-67597.9609\n",
      "-74141.2578\n",
      "-76564.9297\n",
      "-83909.7422\n",
      "-81052.4062\n",
      "-47344.6602\n",
      "-61513.7227\n",
      "-49103.6406\n",
      "-59130.7539\n",
      "-57950.0664\n",
      "-85778.0156\n",
      "-60310.1094\n",
      "-46561.2891\n",
      "-62273.2812\n",
      "-89550.5625\n",
      "-52592.3516\n",
      "-78788.9375\n",
      "-85746.0625\n",
      "-80166.9453\n",
      "-79531.8594\n",
      "-52289.9297\n",
      "-60613.4336\n",
      "-65462.5391\n",
      "-88411.0938\n",
      "-80050.1250\n",
      "-87357.1562\n",
      "-59710.8828\n",
      "-52527.0273\n",
      "-88936.9609\n",
      "-74791.8047\n",
      "-68212.3359\n",
      "-67203.0859\n",
      "-46018.2578\n",
      "-51523.4883\n",
      "-54058.2852\n",
      "-49494.4805\n",
      "-73878.7266\n",
      "-56335.9414\n",
      "-81914.7812\n",
      "-78042.2109\n",
      "-79317.8828\n",
      "-78869.8750\n",
      "-71982.4297\n",
      "-88509.2578\n",
      "-82903.7734\n",
      "-67588.5234\n",
      "-70944.0000\n",
      "-56058.1133\n",
      "-88249.7109\n",
      "-46131.2539\n",
      "-66001.4766\n",
      "-75926.0781\n",
      "-72797.2969\n",
      "-69333.9609\n",
      "-58681.7109\n",
      "-53549.1250\n",
      "-86228.4375\n",
      "-72121.4688\n",
      "-46092.6367\n",
      "-70809.3984\n",
      "-83994.8203\n",
      "-58902.1641\n",
      "-89755.7734\n",
      "-75641.1797\n",
      "-78308.3359\n",
      "-62277.6680\n",
      "-62765.6367\n",
      "-68436.2109\n",
      "-56848.1875\n",
      "-80735.0938\n",
      "-88990.5703\n",
      "-66613.4062\n",
      "-82828.9609\n",
      "-76888.0000\n",
      "-65994.9766\n",
      "-87613.3672\n",
      "-60077.4883\n",
      "-67359.5156\n",
      "-75778.4688\n",
      "-72866.4922\n",
      "-46150.2461\n",
      "-87652.9766\n",
      "-71650.8516\n",
      "-48919.1719\n",
      "-77507.5078\n",
      "-53898.2969\n",
      "-71346.9922\n",
      "-82634.1406\n",
      "-63493.6523\n",
      "-86458.5703\n",
      "-53473.5508\n",
      "-83611.3750\n",
      "-77044.1094\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-8569.8208569335929, 1632.8786332827276]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800 L1: G:Variable containing:\n",
      "-9.4138e+10\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 9.4138e+10\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+05 *\n",
      " -2.0411\n",
      " -2.1077\n",
      " -1.4068\n",
      " -1.9833\n",
      " -1.3233\n",
      " -1.4714\n",
      " -1.9495\n",
      " -1.9078\n",
      " -1.3715\n",
      " -2.1699\n",
      " -1.8244\n",
      " -1.6546\n",
      " -1.7960\n",
      " -1.5254\n",
      " -2.1345\n",
      " -2.1576\n",
      " -2.1247\n",
      " -1.9277\n",
      " -2.0676\n",
      " -1.5711\n",
      " -1.9934\n",
      " -2.1516\n",
      " -2.0431\n",
      " -2.0349\n",
      " -1.1380\n",
      " -1.2359\n",
      " -1.7919\n",
      " -1.9470\n",
      " -2.0496\n",
      " -1.6674\n",
      " -1.1244\n",
      " -1.3446\n",
      " -1.2297\n",
      " -2.1261\n",
      " -1.7902\n",
      " -1.9844\n",
      " -2.1802\n",
      " -1.3345\n",
      " -2.0702\n",
      " -1.9779\n",
      " -1.3827\n",
      " -2.0065\n",
      " -1.6533\n",
      " -1.3882\n",
      " -1.4580\n",
      " -2.1789\n",
      " -1.2747\n",
      " -1.3620\n",
      " -2.1036\n",
      " -1.5773\n",
      " -1.2143\n",
      " -2.0437\n",
      " -1.7857\n",
      " -1.3518\n",
      " -1.6010\n",
      " -1.9098\n",
      " -1.8655\n",
      " -1.4963\n",
      " -1.3217\n",
      " -1.2702\n",
      " -1.9190\n",
      " -1.8112\n",
      " -1.3193\n",
      " -1.6637\n",
      " -1.9139\n",
      " -1.3479\n",
      " -1.4621\n",
      " -1.6083\n",
      " -1.3814\n",
      " -2.0784\n",
      " -1.2702\n",
      " -1.6491\n",
      " -1.6593\n",
      " -1.5887\n",
      " -1.6525\n",
      " -1.6197\n",
      " -1.7240\n",
      " -1.6077\n",
      " -1.8567\n",
      " -1.9744\n",
      " -2.0444\n",
      " -1.4992\n",
      " -1.6504\n",
      " -2.0772\n",
      " -1.9902\n",
      " -2.0002\n",
      " -1.9325\n",
      " -1.9743\n",
      " -1.7966\n",
      " -1.7645\n",
      " -1.1829\n",
      " -1.8293\n",
      " -1.3207\n",
      " -1.3999\n",
      " -1.5435\n",
      " -1.6577\n",
      " -1.8965\n",
      " -1.9900\n",
      " -1.3998\n",
      " -1.3994\n",
      " -1.8545\n",
      " -1.6813\n",
      " -2.0699\n",
      " -1.1229\n",
      " -1.3800\n",
      " -2.1644\n",
      " -1.6415\n",
      " -1.3226\n",
      " -2.1734\n",
      " -2.0810\n",
      " -1.3043\n",
      " -1.3922\n",
      " -1.4062\n",
      " -1.9820\n",
      " -1.7685\n",
      " -1.1084\n",
      " -1.7190\n",
      " -1.7967\n",
      " -1.6919\n",
      " -1.9166\n",
      " -1.3014\n",
      " -2.1176\n",
      " -1.8165\n",
      " -1.4066\n",
      " -1.4954\n",
      " -1.7712\n",
      " -1.6707\n",
      " -1.6857\n",
      " -1.5647\n",
      " -1.8983\n",
      " -2.0793\n",
      " -1.6986\n",
      " -2.0061\n",
      " -1.2650\n",
      " -1.1769\n",
      " -1.4943\n",
      " -1.3519\n",
      " -2.0643\n",
      " -2.0874\n",
      " -1.9558\n",
      " -1.4427\n",
      " -1.4669\n",
      " -1.3200\n",
      " -2.0347\n",
      " -1.9622\n",
      " -1.2675\n",
      " -1.9226\n",
      " -1.9964\n",
      " -1.6714\n",
      " -2.0881\n",
      " -2.1358\n",
      " -2.1100\n",
      " -2.1386\n",
      " -1.2048\n",
      " -1.7294\n",
      " -1.6433\n",
      " -1.4554\n",
      " -1.4178\n",
      " -1.5910\n",
      " -1.8511\n",
      " -1.8387\n",
      " -2.0446\n",
      " -2.0921\n",
      " -1.9845\n",
      " -1.1947\n",
      " -1.3191\n",
      " -1.3726\n",
      " -2.0657\n",
      " -1.5695\n",
      " -1.2250\n",
      " -1.8331\n",
      " -1.4029\n",
      " -1.2177\n",
      " -2.1253\n",
      " -1.6518\n",
      " -1.9796\n",
      " -1.6227\n",
      " -1.1036\n",
      " -1.9838\n",
      " -1.1652\n",
      " -1.9325\n",
      " -1.6037\n",
      " -1.5818\n",
      " -1.9178\n",
      " -1.6611\n",
      " -1.2018\n",
      " -1.2985\n",
      " -1.4833\n",
      " -1.4176\n",
      " -1.7002\n",
      " -1.7816\n",
      " -2.0314\n",
      " -1.3947\n",
      " -1.3112\n",
      " -1.2143\n",
      " -1.3212\n",
      " -1.7504\n",
      " -2.1014\n",
      " -1.7281\n",
      " -1.5063\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-21341.375126953124, 3903.5974624165574]) \n",
      "1000 L1: G:Variable containing:\n",
      "-3.3257e+11\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 3.3257e+11\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+05 *\n",
      " -4.1788\n",
      " -3.9112\n",
      " -2.5328\n",
      " -3.3915\n",
      " -2.1874\n",
      " -3.7472\n",
      " -2.5153\n",
      " -2.6830\n",
      " -4.0375\n",
      " -3.9193\n",
      " -4.0537\n",
      " -3.8548\n",
      " -3.1890\n",
      " -3.4416\n",
      " -3.1049\n",
      " -3.5310\n",
      " -2.5758\n",
      " -2.4073\n",
      " -3.3873\n",
      " -3.3420\n",
      " -3.5779\n",
      " -3.1447\n",
      " -3.6477\n",
      " -2.5427\n",
      " -3.5062\n",
      " -2.7401\n",
      " -3.6708\n",
      " -3.4286\n",
      " -3.7067\n",
      " -3.2754\n",
      " -2.8177\n",
      " -3.0645\n",
      " -2.9415\n",
      " -3.2518\n",
      " -3.3237\n",
      " -2.2355\n",
      " -2.8572\n",
      " -3.0106\n",
      " -2.1935\n",
      " -3.0982\n",
      " -2.6397\n",
      " -3.5781\n",
      " -3.7193\n",
      " -2.6712\n",
      " -3.9093\n",
      " -3.0570\n",
      " -2.5222\n",
      " -3.9537\n",
      " -2.2822\n",
      " -2.8577\n",
      " -2.7116\n",
      " -4.1059\n",
      " -2.8638\n",
      " -3.3744\n",
      " -3.4276\n",
      " -2.3542\n",
      " -3.1969\n",
      " -3.7605\n",
      " -3.7130\n",
      " -2.6656\n",
      " -3.7116\n",
      " -3.5181\n",
      " -3.3286\n",
      " -2.6193\n",
      " -3.7377\n",
      " -2.3445\n",
      " -2.7569\n",
      " -2.9685\n",
      " -3.2604\n",
      " -3.4534\n",
      " -2.3307\n",
      " -2.5258\n",
      " -2.4011\n",
      " -2.6945\n",
      " -2.4928\n",
      " -3.3348\n",
      " -3.0108\n",
      " -2.5203\n",
      " -3.0504\n",
      " -2.1786\n",
      " -4.2135\n",
      " -2.9139\n",
      " -2.7922\n",
      " -2.7976\n",
      " -2.3681\n",
      " -2.2902\n",
      " -3.4329\n",
      " -4.2456\n",
      " -3.9989\n",
      " -2.3292\n",
      " -2.6296\n",
      " -2.9230\n",
      " -2.2581\n",
      " -4.0164\n",
      " -4.1876\n",
      " -3.0200\n",
      " -3.4116\n",
      " -3.4427\n",
      " -3.3459\n",
      " -3.2114\n",
      " -3.2015\n",
      " -2.8622\n",
      " -2.1918\n",
      " -2.4589\n",
      " -3.6345\n",
      " -2.4189\n",
      " -3.3510\n",
      " -3.1610\n",
      " -4.2405\n",
      " -2.2375\n",
      " -3.4513\n",
      " -3.8980\n",
      " -2.8997\n",
      " -2.9735\n",
      " -3.3410\n",
      " -2.5625\n",
      " -2.3203\n",
      " -2.9343\n",
      " -2.4861\n",
      " -4.1927\n",
      " -3.4801\n",
      " -4.2484\n",
      " -2.4988\n",
      " -3.2771\n",
      " -3.1574\n",
      " -4.0400\n",
      " -3.0706\n",
      " -3.1383\n",
      " -2.2733\n",
      " -2.6164\n",
      " -3.3019\n",
      " -3.8826\n",
      " -3.5420\n",
      " -3.0168\n",
      " -3.7002\n",
      " -4.0648\n",
      " -3.4418\n",
      " -4.2647\n",
      " -2.7349\n",
      " -3.5776\n",
      " -3.1929\n",
      " -4.1468\n",
      " -2.3702\n",
      " -3.0877\n",
      " -3.0188\n",
      " -4.0262\n",
      " -3.9335\n",
      " -2.9894\n",
      " -3.7639\n",
      " -3.3280\n",
      " -2.6263\n",
      " -3.5021\n",
      " -2.4626\n",
      " -3.8697\n",
      " -3.4046\n",
      " -2.9401\n",
      " -2.9309\n",
      " -2.8261\n",
      " -4.1895\n",
      " -2.9402\n",
      " -3.8199\n",
      " -3.0677\n",
      " -2.6675\n",
      " -4.2438\n",
      " -2.5151\n",
      " -3.9723\n",
      " -3.5840\n",
      " -4.2440\n",
      " -4.2082\n",
      " -2.6979\n",
      " -3.7954\n",
      " -4.2565\n",
      " -3.6302\n",
      " -3.1675\n",
      " -3.7376\n",
      " -3.4047\n",
      " -2.4401\n",
      " -2.2871\n",
      " -2.4667\n",
      " -4.1453\n",
      " -4.1534\n",
      " -2.4161\n",
      " -2.5008\n",
      " -2.2661\n",
      " -3.4928\n",
      " -2.4999\n",
      " -2.9223\n",
      " -3.1085\n",
      " -2.1722\n",
      " -3.2562\n",
      " -3.0200\n",
      " -3.2177\n",
      " -3.0259\n",
      " -3.5591\n",
      " -2.1751\n",
      " -2.5489\n",
      " -3.5177\n",
      " -3.8412\n",
      " -2.5582\n",
      " -3.2163\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-40080.160908203128, 7510.7427976443068]) \n",
      "1200 L1: G:Variable containing:\n",
      "-9.6369e+11\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 9.6369e+11\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+05 *\n",
      " -3.9190\n",
      " -4.9070\n",
      " -4.5672\n",
      " -5.0040\n",
      " -4.4684\n",
      " -4.8644\n",
      " -7.0487\n",
      " -3.9684\n",
      " -5.7952\n",
      " -5.5479\n",
      " -5.9919\n",
      " -4.8679\n",
      " -7.1520\n",
      " -5.6838\n",
      " -4.0816\n",
      " -4.5282\n",
      " -4.3583\n",
      " -6.8166\n",
      " -6.9248\n",
      " -5.7292\n",
      " -4.1680\n",
      " -6.5721\n",
      " -4.0141\n",
      " -6.4657\n",
      " -3.6900\n",
      " -4.9014\n",
      " -5.3820\n",
      " -6.8150\n",
      " -4.6098\n",
      " -5.6908\n",
      " -5.5154\n",
      " -6.1753\n",
      " -5.8825\n",
      " -3.7767\n",
      " -4.2248\n",
      " -3.8591\n",
      " -5.4334\n",
      " -5.1551\n",
      " -5.1843\n",
      " -6.4596\n",
      " -4.7353\n",
      " -6.2863\n",
      " -7.0759\n",
      " -5.4858\n",
      " -4.7499\n",
      " -6.0412\n",
      " -6.1010\n",
      " -4.7590\n",
      " -3.6655\n",
      " -6.0899\n",
      " -5.7168\n",
      " -5.8458\n",
      " -7.2365\n",
      " -6.1111\n",
      " -5.3303\n",
      " -5.9945\n",
      " -4.9002\n",
      " -4.8148\n",
      " -4.6605\n",
      " -5.8713\n",
      " -4.7216\n",
      " -4.5945\n",
      " -5.9213\n",
      " -6.2739\n",
      " -6.7201\n",
      " -4.8328\n",
      " -5.6040\n",
      " -4.2864\n",
      " -5.6397\n",
      " -6.8034\n",
      " -6.7849\n",
      " -5.6486\n",
      " -6.7055\n",
      " -3.9491\n",
      " -6.3540\n",
      " -4.2534\n",
      " -3.7962\n",
      " -4.0488\n",
      " -6.1749\n",
      " -6.5861\n",
      " -6.5279\n",
      " -5.1137\n",
      " -4.4901\n",
      " -6.8838\n",
      " -3.7966\n",
      " -4.8624\n",
      " -5.3863\n",
      " -4.2914\n",
      " -4.9734\n",
      " -4.3274\n",
      " -7.1124\n",
      " -3.7412\n",
      " -5.2619\n",
      " -6.7872\n",
      " -5.1694\n",
      " -5.9660\n",
      " -3.9496\n",
      " -5.6555\n",
      " -4.7779\n",
      " -6.5640\n",
      " -3.8452\n",
      " -5.5329\n",
      " -5.8740\n",
      " -3.9314\n",
      " -5.4366\n",
      " -7.1607\n",
      " -5.1899\n",
      " -6.3938\n",
      " -4.8888\n",
      " -4.4376\n",
      " -6.7539\n",
      " -5.3491\n",
      " -5.4807\n",
      " -5.3199\n",
      " -6.5664\n",
      " -4.7974\n",
      " -4.2856\n",
      " -6.4338\n",
      " -6.8744\n",
      " -5.3845\n",
      " -5.5532\n",
      " -4.6575\n",
      " -3.6743\n",
      " -6.1557\n",
      " -5.8493\n",
      " -4.7868\n",
      " -4.3276\n",
      " -4.4501\n",
      " -6.0149\n",
      " -3.8510\n",
      " -4.9090\n",
      " -5.6046\n",
      " -7.1616\n",
      " -6.5477\n",
      " -6.0329\n",
      " -5.2804\n",
      " -5.9385\n",
      " -7.1352\n",
      " -5.6286\n",
      " -4.7882\n",
      " -4.1685\n",
      " -4.7609\n",
      " -5.9487\n",
      " -7.1166\n",
      " -4.6297\n",
      " -5.6625\n",
      " -4.2210\n",
      " -3.8777\n",
      " -7.1529\n",
      " -4.8314\n",
      " -5.0096\n",
      " -6.1887\n",
      " -6.5816\n",
      " -6.8803\n",
      " -5.9896\n",
      " -5.4947\n",
      " -6.5477\n",
      " -5.5279\n",
      " -4.0666\n",
      " -4.8732\n",
      " -4.9511\n",
      " -6.5011\n",
      " -5.6070\n",
      " -5.0902\n",
      " -5.2232\n",
      " -3.6738\n",
      " -4.4944\n",
      " -4.9712\n",
      " -4.4134\n",
      " -4.9393\n",
      " -7.0557\n",
      " -4.3672\n",
      " -4.6119\n",
      " -5.4401\n",
      " -4.8424\n",
      " -7.1877\n",
      " -4.7230\n",
      " -5.8372\n",
      " -4.8801\n",
      " -5.7261\n",
      " -6.4243\n",
      " -5.7070\n",
      " -6.0299\n",
      " -4.1055\n",
      " -4.8987\n",
      " -4.6193\n",
      " -7.1932\n",
      " -7.1615\n",
      " -4.4661\n",
      " -5.8951\n",
      " -4.0525\n",
      " -7.0488\n",
      " -6.9258\n",
      " -6.2486\n",
      " -6.8832\n",
      " -4.4938\n",
      " -4.0440\n",
      " -3.9998\n",
      " -6.9571\n",
      " -4.6287\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-68242.556992187499, 12704.991723766818]) \n",
      "1400 L1: G:Variable containing:\n",
      "-2.3332e+12\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 2.3332e+12\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+06 *\n",
      " -0.5708\n",
      " -0.7084\n",
      " -1.1015\n",
      " -0.5931\n",
      " -0.8129\n",
      " -0.6444\n",
      " -0.6590\n",
      " -0.7847\n",
      " -0.7005\n",
      " -0.9396\n",
      " -0.6090\n",
      " -0.6320\n",
      " -0.9329\n",
      " -0.5636\n",
      " -0.8855\n",
      " -0.9061\n",
      " -1.0939\n",
      " -0.7801\n",
      " -0.8203\n",
      " -0.6585\n",
      " -0.6361\n",
      " -1.1124\n",
      " -0.8433\n",
      " -0.8561\n",
      " -0.7305\n",
      " -0.9740\n",
      " -0.6625\n",
      " -0.8683\n",
      " -0.9790\n",
      " -0.5700\n",
      " -1.0197\n",
      " -0.8557\n",
      " -0.7258\n",
      " -0.5632\n",
      " -0.7869\n",
      " -0.7374\n",
      " -0.7414\n",
      " -0.8407\n",
      " -0.7772\n",
      " -0.9578\n",
      " -0.7577\n",
      " -0.9084\n",
      " -0.6986\n",
      " -0.6493\n",
      " -1.0763\n",
      " -0.6744\n",
      " -0.9375\n",
      " -1.0668\n",
      " -0.8242\n",
      " -0.6631\n",
      " -0.6821\n",
      " -0.9614\n",
      " -0.6700\n",
      " -0.9706\n",
      " -0.9514\n",
      " -0.8152\n",
      " -0.7296\n",
      " -0.8001\n",
      " -0.7691\n",
      " -0.9219\n",
      " -0.5937\n",
      " -0.5864\n",
      " -0.9953\n",
      " -0.9324\n",
      " -1.0114\n",
      " -0.7311\n",
      " -0.8417\n",
      " -1.0974\n",
      " -0.6108\n",
      " -1.1033\n",
      " -1.0876\n",
      " -0.6284\n",
      " -0.6860\n",
      " -0.9652\n",
      " -0.6494\n",
      " -0.6511\n",
      " -0.9183\n",
      " -0.5996\n",
      " -0.9266\n",
      " -0.6478\n",
      " -0.9339\n",
      " -0.7763\n",
      " -0.7896\n",
      " -1.0631\n",
      " -0.9987\n",
      " -0.8308\n",
      " -0.8568\n",
      " -1.0356\n",
      " -1.0035\n",
      " -0.8783\n",
      " -1.0945\n",
      " -1.0651\n",
      " -0.9750\n",
      " -1.1098\n",
      " -0.8922\n",
      " -0.8305\n",
      " -0.7644\n",
      " -0.8721\n",
      " -0.8020\n",
      " -0.6227\n",
      " -0.9300\n",
      " -0.9321\n",
      " -0.6011\n",
      " -0.8264\n",
      " -0.6870\n",
      " -0.8458\n",
      " -0.6641\n",
      " -0.9253\n",
      " -1.0534\n",
      " -1.1063\n",
      " -0.8731\n",
      " -1.0339\n",
      " -1.0588\n",
      " -0.9997\n",
      " -0.9661\n",
      " -0.7108\n",
      " -0.7751\n",
      " -0.7378\n",
      " -0.5854\n",
      " -0.6604\n",
      " -0.7827\n",
      " -0.8617\n",
      " -1.1011\n",
      " -0.8497\n",
      " -0.9897\n",
      " -1.0806\n",
      " -1.1002\n",
      " -0.8269\n",
      " -0.7033\n",
      " -0.8044\n",
      " -1.0964\n",
      " -0.8933\n",
      " -0.8011\n",
      " -1.0830\n",
      " -1.0914\n",
      " -1.0190\n",
      " -0.6365\n",
      " -0.9098\n",
      " -1.1034\n",
      " -1.0467\n",
      " -0.9235\n",
      " -0.5681\n",
      " -0.6479\n",
      " -0.6152\n",
      " -1.0884\n",
      " -0.8950\n",
      " -0.9305\n",
      " -1.0275\n",
      " -1.0282\n",
      " -0.7421\n",
      " -1.0435\n",
      " -0.8586\n",
      " -0.7959\n",
      " -1.0429\n",
      " -1.0514\n",
      " -0.8864\n",
      " -0.9571\n",
      " -0.9374\n",
      " -1.0452\n",
      " -0.7522\n",
      " -1.0268\n",
      " -0.9433\n",
      " -1.0137\n",
      " -0.6625\n",
      " -0.7881\n",
      " -0.6948\n",
      " -0.9548\n",
      " -0.6244\n",
      " -0.7241\n",
      " -0.7278\n",
      " -1.0775\n",
      " -0.6342\n",
      " -0.7815\n",
      " -1.0115\n",
      " -1.0536\n",
      " -0.6693\n",
      " -0.5809\n",
      " -0.7225\n",
      " -0.7347\n",
      " -0.6950\n",
      " -0.7027\n",
      " -0.7996\n",
      " -0.6034\n",
      " -1.0439\n",
      " -0.5885\n",
      " -1.0486\n",
      " -1.0875\n",
      " -0.5935\n",
      " -0.8157\n",
      " -0.7658\n",
      " -0.6152\n",
      " -0.6225\n",
      " -0.6961\n",
      " -0.8793\n",
      " -0.7503\n",
      " -0.9518\n",
      " -0.6355\n",
      " -0.6902\n",
      " -1.0510\n",
      " -0.6718\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-106008.2368359375, 20691.809610156601]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600 L1: G:Variable containing:\n",
      "-4.8214e+12\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 4.8214e+12\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+06 *\n",
      " -1.4358\n",
      " -1.2467\n",
      " -1.5715\n",
      " -1.2286\n",
      " -1.0606\n",
      " -1.3005\n",
      " -1.1837\n",
      " -0.8223\n",
      " -0.9894\n",
      " -1.3156\n",
      " -1.5535\n",
      " -1.1641\n",
      " -0.9413\n",
      " -1.5635\n",
      " -0.9916\n",
      " -1.0026\n",
      " -1.3409\n",
      " -0.8130\n",
      " -1.5647\n",
      " -0.8975\n",
      " -1.4550\n",
      " -0.9710\n",
      " -1.1281\n",
      " -1.5275\n",
      " -1.4763\n",
      " -1.5698\n",
      " -1.1033\n",
      " -1.3851\n",
      " -1.0718\n",
      " -1.5819\n",
      " -1.3922\n",
      " -1.5134\n",
      " -1.4612\n",
      " -1.5783\n",
      " -0.8437\n",
      " -1.0167\n",
      " -1.4394\n",
      " -1.2346\n",
      " -0.8665\n",
      " -1.0067\n",
      " -0.8659\n",
      " -0.9103\n",
      " -0.9194\n",
      " -1.3324\n",
      " -1.5288\n",
      " -1.0671\n",
      " -1.1260\n",
      " -1.5906\n",
      " -1.1770\n",
      " -1.0244\n",
      " -1.4655\n",
      " -1.5420\n",
      " -0.9299\n",
      " -1.1839\n",
      " -1.3414\n",
      " -0.9065\n",
      " -1.4973\n",
      " -1.0336\n",
      " -1.2095\n",
      " -0.9050\n",
      " -1.5280\n",
      " -1.0451\n",
      " -1.5217\n",
      " -0.9621\n",
      " -1.1796\n",
      " -0.8927\n",
      " -1.1898\n",
      " -0.8207\n",
      " -1.4983\n",
      " -1.5312\n",
      " -1.1484\n",
      " -1.6012\n",
      " -1.5454\n",
      " -0.9877\n",
      " -0.8668\n",
      " -0.9957\n",
      " -0.9879\n",
      " -1.1279\n",
      " -1.1531\n",
      " -0.8438\n",
      " -1.1082\n",
      " -1.2743\n",
      " -1.1016\n",
      " -1.2849\n",
      " -1.0066\n",
      " -1.0370\n",
      " -1.4428\n",
      " -1.3839\n",
      " -1.1098\n",
      " -1.5978\n",
      " -1.2868\n",
      " -0.9758\n",
      " -1.1194\n",
      " -1.4548\n",
      " -1.1979\n",
      " -0.8271\n",
      " -1.1527\n",
      " -1.0860\n",
      " -0.8241\n",
      " -0.8780\n",
      " -1.2969\n",
      " -1.5976\n",
      " -1.0225\n",
      " -1.2110\n",
      " -1.5597\n",
      " -1.5799\n",
      " -1.3187\n",
      " -0.8536\n",
      " -1.5592\n",
      " -1.2218\n",
      " -1.4334\n",
      " -0.8609\n",
      " -0.8870\n",
      " -1.3322\n",
      " -1.5650\n",
      " -1.3318\n",
      " -0.9248\n",
      " -0.9533\n",
      " -1.5394\n",
      " -1.3575\n",
      " -1.5240\n",
      " -1.3422\n",
      " -1.1679\n",
      " -1.4451\n",
      " -1.1429\n",
      " -1.2287\n",
      " -1.3695\n",
      " -1.5944\n",
      " -1.5784\n",
      " -1.4843\n",
      " -1.4634\n",
      " -0.9640\n",
      " -1.5162\n",
      " -1.3010\n",
      " -1.0043\n",
      " -0.8343\n",
      " -1.1500\n",
      " -1.2664\n",
      " -1.5490\n",
      " -1.5341\n",
      " -0.9414\n",
      " -1.0567\n",
      " -1.5750\n",
      " -1.4938\n",
      " -1.4478\n",
      " -0.8552\n",
      " -1.2297\n",
      " -1.3423\n",
      " -0.8166\n",
      " -1.0470\n",
      " -1.1803\n",
      " -0.9126\n",
      " -1.3520\n",
      " -0.8542\n",
      " -1.2507\n",
      " -1.2477\n",
      " -1.3824\n",
      " -0.8421\n",
      " -1.3681\n",
      " -0.8570\n",
      " -0.8610\n",
      " -0.8920\n",
      " -0.8778\n",
      " -1.4227\n",
      " -1.5530\n",
      " -1.0739\n",
      " -1.2197\n",
      " -1.0159\n",
      " -1.2722\n",
      " -1.5152\n",
      " -0.9005\n",
      " -1.1384\n",
      " -1.1304\n",
      " -1.1543\n",
      " -0.9376\n",
      " -1.0378\n",
      " -0.8584\n",
      " -0.9603\n",
      " -1.4234\n",
      " -1.4516\n",
      " -1.0728\n",
      " -1.3295\n",
      " -1.2928\n",
      " -0.9514\n",
      " -0.8161\n",
      " -0.8250\n",
      " -1.2000\n",
      " -1.4509\n",
      " -1.1077\n",
      " -0.8554\n",
      " -1.1258\n",
      " -1.3747\n",
      " -1.4263\n",
      " -1.6061\n",
      " -1.5253\n",
      " -0.9836\n",
      " -1.2236\n",
      " -1.2383\n",
      " -1.2326\n",
      " -1.0131\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-152125.913203125, 31056.197120520414]) \n",
      "1800 L1: G:Variable containing:\n",
      "-8.7863e+12\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 8.7863e+12\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+06 *\n",
      " -2.0498\n",
      " -2.0930\n",
      " -1.9760\n",
      " -1.3400\n",
      " -2.1265\n",
      " -1.9055\n",
      " -1.7969\n",
      " -1.4434\n",
      " -1.8886\n",
      " -1.5997\n",
      " -1.5012\n",
      " -1.4562\n",
      " -2.0336\n",
      " -1.6600\n",
      " -1.6525\n",
      " -2.1622\n",
      " -1.8504\n",
      " -1.5748\n",
      " -1.1658\n",
      " -1.2091\n",
      " -1.5406\n",
      " -2.1289\n",
      " -1.3725\n",
      " -1.5996\n",
      " -1.9553\n",
      " -1.3355\n",
      " -1.5480\n",
      " -1.6709\n",
      " -1.7811\n",
      " -1.4941\n",
      " -1.5482\n",
      " -2.1346\n",
      " -1.4298\n",
      " -1.4764\n",
      " -1.7999\n",
      " -1.5298\n",
      " -1.3444\n",
      " -1.8261\n",
      " -1.6913\n",
      " -1.7348\n",
      " -1.7951\n",
      " -1.2204\n",
      " -1.1477\n",
      " -1.3097\n",
      " -1.1428\n",
      " -1.2024\n",
      " -1.1580\n",
      " -1.9210\n",
      " -2.0139\n",
      " -1.1931\n",
      " -1.4457\n",
      " -2.1182\n",
      " -1.3103\n",
      " -1.1890\n",
      " -1.5885\n",
      " -1.5114\n",
      " -1.4692\n",
      " -1.7110\n",
      " -2.1361\n",
      " -1.1711\n",
      " -1.6167\n",
      " -1.8592\n",
      " -2.1482\n",
      " -1.8101\n",
      " -1.3852\n",
      " -1.1365\n",
      " -1.5722\n",
      " -1.4019\n",
      " -1.3882\n",
      " -2.1728\n",
      " -2.1918\n",
      " -1.9273\n",
      " -1.4415\n",
      " -1.1563\n",
      " -1.6179\n",
      " -1.4457\n",
      " -1.4007\n",
      " -1.1961\n",
      " -1.2047\n",
      " -2.1005\n",
      " -1.1670\n",
      " -1.3675\n",
      " -1.1698\n",
      " -2.0140\n",
      " -1.6973\n",
      " -1.1405\n",
      " -1.7387\n",
      " -2.0858\n",
      " -1.4516\n",
      " -2.1907\n",
      " -2.1474\n",
      " -2.0334\n",
      " -1.7027\n",
      " -1.4908\n",
      " -2.0152\n",
      " -1.3694\n",
      " -1.3656\n",
      " -1.9330\n",
      " -2.0162\n",
      " -1.5224\n",
      " -2.1267\n",
      " -1.7826\n",
      " -1.5618\n",
      " -2.1414\n",
      " -1.7532\n",
      " -1.7674\n",
      " -1.6593\n",
      " -1.9410\n",
      " -1.2913\n",
      " -1.3903\n",
      " -1.3564\n",
      " -1.9755\n",
      " -1.6988\n",
      " -2.1730\n",
      " -1.9922\n",
      " -2.0919\n",
      " -1.9559\n",
      " -1.1668\n",
      " -2.0334\n",
      " -1.4494\n",
      " -1.6265\n",
      " -1.8965\n",
      " -1.3475\n",
      " -1.9872\n",
      " -1.4416\n",
      " -1.3895\n",
      " -1.6595\n",
      " -1.1990\n",
      " -1.9901\n",
      " -1.7011\n",
      " -1.9930\n",
      " -1.8845\n",
      " -1.3010\n",
      " -1.2935\n",
      " -1.9238\n",
      " -1.4993\n",
      " -1.1113\n",
      " -1.3680\n",
      " -2.0030\n",
      " -2.0059\n",
      " -1.6943\n",
      " -1.1503\n",
      " -1.6246\n",
      " -1.5290\n",
      " -1.7470\n",
      " -1.6562\n",
      " -1.6367\n",
      " -2.0750\n",
      " -1.3602\n",
      " -1.5099\n",
      " -1.3739\n",
      " -1.2228\n",
      " -1.3160\n",
      " -1.6394\n",
      " -1.9789\n",
      " -1.9561\n",
      " -1.8388\n",
      " -1.6472\n",
      " -1.6966\n",
      " -1.5369\n",
      " -2.0524\n",
      " -1.3901\n",
      " -1.3458\n",
      " -1.5745\n",
      " -1.2419\n",
      " -1.3651\n",
      " -1.8373\n",
      " -1.3271\n",
      " -1.4644\n",
      " -1.3067\n",
      " -1.1121\n",
      " -1.2369\n",
      " -1.8748\n",
      " -1.9762\n",
      " -2.1757\n",
      " -1.4833\n",
      " -1.7688\n",
      " -1.8589\n",
      " -1.4422\n",
      " -1.4765\n",
      " -1.1903\n",
      " -1.9573\n",
      " -2.1294\n",
      " -1.7143\n",
      " -2.1944\n",
      " -1.9613\n",
      " -1.8904\n",
      " -1.8940\n",
      " -1.6791\n",
      " -1.4348\n",
      " -1.3174\n",
      " -1.1423\n",
      " -1.2183\n",
      " -1.1847\n",
      " -1.5284\n",
      " -1.3063\n",
      " -1.3154\n",
      " -1.7917\n",
      " -1.4700\n",
      " -1.2946\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-205785.41398437499, 39795.631260994494]) \n",
      "2000 L1: G:Variable containing:\n",
      "-1.5468e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 1.5468e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+06 *\n",
      " -2.1917\n",
      " -2.0885\n",
      " -2.9077\n",
      " -2.2703\n",
      " -1.5529\n",
      " -2.3849\n",
      " -1.5475\n",
      " -2.3455\n",
      " -2.3171\n",
      " -2.8930\n",
      " -2.5734\n",
      " -2.1185\n",
      " -1.9995\n",
      " -2.7860\n",
      " -1.8152\n",
      " -2.4563\n",
      " -2.1880\n",
      " -1.4727\n",
      " -2.3641\n",
      " -2.9081\n",
      " -2.1975\n",
      " -1.6111\n",
      " -1.5263\n",
      " -2.5783\n",
      " -2.4129\n",
      " -2.7145\n",
      " -1.7712\n",
      " -2.8281\n",
      " -1.6025\n",
      " -2.1997\n",
      " -1.4762\n",
      " -2.8921\n",
      " -2.2309\n",
      " -2.0153\n",
      " -2.4083\n",
      " -2.3365\n",
      " -1.8824\n",
      " -1.7841\n",
      " -2.7346\n",
      " -1.9199\n",
      " -2.2427\n",
      " -1.7934\n",
      " -2.3938\n",
      " -1.7915\n",
      " -2.1089\n",
      " -1.7608\n",
      " -2.6916\n",
      " -1.5557\n",
      " -2.7006\n",
      " -1.9730\n",
      " -2.4292\n",
      " -2.0735\n",
      " -1.6276\n",
      " -1.4979\n",
      " -1.6817\n",
      " -2.1487\n",
      " -2.8609\n",
      " -1.9113\n",
      " -2.7494\n",
      " -1.7193\n",
      " -2.3007\n",
      " -1.8031\n",
      " -2.0163\n",
      " -2.3655\n",
      " -2.1986\n",
      " -2.4194\n",
      " -2.7691\n",
      " -1.6790\n",
      " -2.6588\n",
      " -1.8069\n",
      " -1.5134\n",
      " -1.7983\n",
      " -2.3471\n",
      " -1.9769\n",
      " -2.8637\n",
      " -1.5098\n",
      " -2.2188\n",
      " -2.5148\n",
      " -2.8070\n",
      " -1.6095\n",
      " -1.9667\n",
      " -2.4836\n",
      " -2.0578\n",
      " -2.3908\n",
      " -2.7942\n",
      " -1.7426\n",
      " -2.2693\n",
      " -2.3156\n",
      " -2.0258\n",
      " -1.4969\n",
      " -2.0987\n",
      " -1.9108\n",
      " -2.7630\n",
      " -2.4673\n",
      " -1.6586\n",
      " -2.2566\n",
      " -2.0332\n",
      " -2.5248\n",
      " -1.8078\n",
      " -2.6991\n",
      " -2.0099\n",
      " -2.1096\n",
      " -1.5343\n",
      " -2.0637\n",
      " -2.4883\n",
      " -2.6887\n",
      " -2.8235\n",
      " -2.7643\n",
      " -2.5845\n",
      " -2.3643\n",
      " -2.6844\n",
      " -1.9926\n",
      " -1.5822\n",
      " -1.7718\n",
      " -2.1035\n",
      " -1.7519\n",
      " -1.8673\n",
      " -2.8097\n",
      " -2.2204\n",
      " -2.0802\n",
      " -2.4182\n",
      " -2.6585\n",
      " -1.5583\n",
      " -2.1118\n",
      " -1.8279\n",
      " -1.9230\n",
      " -2.3774\n",
      " -2.2465\n",
      " -1.6311\n",
      " -2.1161\n",
      " -2.6548\n",
      " -2.1765\n",
      " -1.9341\n",
      " -2.4115\n",
      " -1.5540\n",
      " -1.4813\n",
      " -1.5503\n",
      " -2.2422\n",
      " -2.2560\n",
      " -2.0740\n",
      " -2.5977\n",
      " -2.0848\n",
      " -1.5304\n",
      " -1.6541\n",
      " -1.6369\n",
      " -1.6554\n",
      " -2.6355\n",
      " -1.6079\n",
      " -2.1234\n",
      " -2.0667\n",
      " -2.8213\n",
      " -2.4179\n",
      " -1.4606\n",
      " -1.6816\n",
      " -2.9067\n",
      " -2.2073\n",
      " -1.5526\n",
      " -2.6110\n",
      " -1.8256\n",
      " -1.9045\n",
      " -1.8948\n",
      " -2.2247\n",
      " -1.8237\n",
      " -2.5936\n",
      " -2.7296\n",
      " -2.5523\n",
      " -2.6492\n",
      " -2.2588\n",
      " -2.5788\n",
      " -1.7907\n",
      " -2.4269\n",
      " -2.0314\n",
      " -1.7413\n",
      " -2.7696\n",
      " -2.1934\n",
      " -2.2783\n",
      " -2.2591\n",
      " -2.2540\n",
      " -2.0009\n",
      " -2.2474\n",
      " -2.4183\n",
      " -2.1772\n",
      " -2.6441\n",
      " -2.0389\n",
      " -2.1497\n",
      " -2.3085\n",
      " -2.8060\n",
      " -2.1053\n",
      " -1.6101\n",
      " -1.6757\n",
      " -1.9582\n",
      " -2.3332\n",
      " -2.0259\n",
      " -2.8782\n",
      " -2.0802\n",
      " -2.5009\n",
      " -1.9373\n",
      " -2.4603\n",
      " -2.4551\n",
      " -1.8380\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-273397.38851562497, 50952.904696323843]) \n",
      "2200 L1: G:Variable containing:\n",
      "-2.6776e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 2.6776e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+06 *\n",
      " -3.5378\n",
      " -2.3792\n",
      " -3.6046\n",
      " -3.1099\n",
      " -2.8241\n",
      " -2.3003\n",
      " -3.1716\n",
      " -1.9323\n",
      " -3.2473\n",
      " -3.0161\n",
      " -3.0293\n",
      " -3.6601\n",
      " -3.2223\n",
      " -3.1403\n",
      " -3.4891\n",
      " -2.0117\n",
      " -2.6313\n",
      " -3.3068\n",
      " -2.9520\n",
      " -3.1837\n",
      " -2.8316\n",
      " -2.6533\n",
      " -3.1436\n",
      " -3.0122\n",
      " -2.0931\n",
      " -2.8778\n",
      " -2.5754\n",
      " -2.3778\n",
      " -2.0315\n",
      " -2.9751\n",
      " -3.4856\n",
      " -1.9301\n",
      " -2.1553\n",
      " -2.2841\n",
      " -2.9382\n",
      " -3.4490\n",
      " -3.0864\n",
      " -3.5900\n",
      " -3.1018\n",
      " -2.0701\n",
      " -2.9878\n",
      " -2.9732\n",
      " -2.7539\n",
      " -3.3679\n",
      " -3.1474\n",
      " -3.2686\n",
      " -2.6543\n",
      " -2.7041\n",
      " -2.8101\n",
      " -2.1895\n",
      " -3.3640\n",
      " -3.1513\n",
      " -3.1937\n",
      " -3.6232\n",
      " -2.7959\n",
      " -3.6438\n",
      " -3.3225\n",
      " -3.5324\n",
      " -2.3770\n",
      " -2.1069\n",
      " -2.2912\n",
      " -2.2514\n",
      " -3.3160\n",
      " -2.9269\n",
      " -3.3566\n",
      " -1.9902\n",
      " -2.5022\n",
      " -2.1654\n",
      " -3.5896\n",
      " -3.4384\n",
      " -2.2460\n",
      " -3.7074\n",
      " -2.2952\n",
      " -2.1774\n",
      " -2.7703\n",
      " -3.4561\n",
      " -3.5070\n",
      " -2.6576\n",
      " -3.5142\n",
      " -3.2433\n",
      " -2.5625\n",
      " -2.0743\n",
      " -3.3588\n",
      " -3.2695\n",
      " -3.6091\n",
      " -3.5445\n",
      " -2.6091\n",
      " -2.7956\n",
      " -2.1281\n",
      " -3.2064\n",
      " -2.4468\n",
      " -2.6728\n",
      " -2.8323\n",
      " -3.7091\n",
      " -2.8636\n",
      " -2.8083\n",
      " -2.9116\n",
      " -3.5980\n",
      " -2.1345\n",
      " -2.9086\n",
      " -2.3230\n",
      " -3.5310\n",
      " -3.1778\n",
      " -2.0808\n",
      " -3.0584\n",
      " -2.7959\n",
      " -2.3978\n",
      " -3.6829\n",
      " -3.1485\n",
      " -2.9109\n",
      " -2.4652\n",
      " -2.7069\n",
      " -2.7103\n",
      " -3.6456\n",
      " -2.6625\n",
      " -1.8745\n",
      " -3.4989\n",
      " -2.5295\n",
      " -3.4625\n",
      " -3.7075\n",
      " -2.0921\n",
      " -2.1282\n",
      " -3.3263\n",
      " -2.5060\n",
      " -2.0763\n",
      " -2.2024\n",
      " -2.6383\n",
      " -2.5385\n",
      " -2.4332\n",
      " -3.4732\n",
      " -2.6499\n",
      " -2.1529\n",
      " -1.9034\n",
      " -1.9424\n",
      " -3.2878\n",
      " -3.5562\n",
      " -3.3402\n",
      " -1.8718\n",
      " -2.9711\n",
      " -3.6720\n",
      " -3.4566\n",
      " -2.4418\n",
      " -3.5774\n",
      " -2.9918\n",
      " -3.2740\n",
      " -1.9439\n",
      " -3.5177\n",
      " -3.4092\n",
      " -2.1174\n",
      " -2.3928\n",
      " -3.3928\n",
      " -2.4498\n",
      " -2.0231\n",
      " -3.1558\n",
      " -2.5298\n",
      " -2.0153\n",
      " -2.0639\n",
      " -3.6918\n",
      " -2.7830\n",
      " -2.7233\n",
      " -1.9919\n",
      " -2.6620\n",
      " -3.5720\n",
      " -2.1018\n",
      " -2.5215\n",
      " -3.6202\n",
      " -2.7186\n",
      " -3.3917\n",
      " -2.2645\n",
      " -2.0038\n",
      " -3.5001\n",
      " -3.6161\n",
      " -2.5685\n",
      " -3.3644\n",
      " -3.4406\n",
      " -3.2425\n",
      " -2.1559\n",
      " -3.3443\n",
      " -2.6555\n",
      " -1.9720\n",
      " -3.5385\n",
      " -2.8265\n",
      " -2.2134\n",
      " -2.7673\n",
      " -2.3844\n",
      " -3.5603\n",
      " -2.0459\n",
      " -2.1122\n",
      " -3.5635\n",
      " -3.2341\n",
      " -2.6565\n",
      " -2.1124\n",
      " -3.2563\n",
      " -2.8799\n",
      " -2.5776\n",
      " -2.3706\n",
      " -3.4113\n",
      " -2.0462\n",
      " -2.3308\n",
      " -3.5822\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-359210.88070312497, 69627.257957913229]) \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2400 L1: G:Variable containing:\n",
      "-3.9879e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 3.9879e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+06 *\n",
      " -4.4171\n",
      " -3.9589\n",
      " -4.3732\n",
      " -2.8904\n",
      " -2.3382\n",
      " -4.5675\n",
      " -3.6888\n",
      " -4.2594\n",
      " -3.2786\n",
      " -3.7460\n",
      " -2.6871\n",
      " -3.6091\n",
      " -2.7878\n",
      " -3.8046\n",
      " -4.2516\n",
      " -2.7675\n",
      " -2.6098\n",
      " -4.3603\n",
      " -4.2507\n",
      " -2.7544\n",
      " -2.5701\n",
      " -4.6084\n",
      " -2.6247\n",
      " -3.9484\n",
      " -3.8011\n",
      " -3.1214\n",
      " -2.9959\n",
      " -3.7121\n",
      " -2.5220\n",
      " -3.3506\n",
      " -2.5286\n",
      " -3.3555\n",
      " -3.2577\n",
      " -3.1453\n",
      " -3.4511\n",
      " -2.7718\n",
      " -3.8294\n",
      " -2.7342\n",
      " -4.1029\n",
      " -4.0799\n",
      " -4.5810\n",
      " -4.4249\n",
      " -2.8681\n",
      " -3.0946\n",
      " -4.4434\n",
      " -4.4272\n",
      " -3.0816\n",
      " -2.9079\n",
      " -3.2002\n",
      " -4.0232\n",
      " -4.3605\n",
      " -2.8410\n",
      " -3.3017\n",
      " -2.9750\n",
      " -2.8164\n",
      " -3.1785\n",
      " -3.3715\n",
      " -3.0615\n",
      " -4.6197\n",
      " -4.4058\n",
      " -4.1391\n",
      " -3.7012\n",
      " -2.6272\n",
      " -2.8374\n",
      " -4.5097\n",
      " -3.6285\n",
      " -3.2314\n",
      " -2.4806\n",
      " -3.4954\n",
      " -3.4135\n",
      " -3.2549\n",
      " -4.0561\n",
      " -2.3418\n",
      " -3.0599\n",
      " -4.3606\n",
      " -2.6539\n",
      " -2.9594\n",
      " -2.5776\n",
      " -2.3897\n",
      " -4.0651\n",
      " -2.6727\n",
      " -3.0532\n",
      " -2.8701\n",
      " -4.4987\n",
      " -4.0043\n",
      " -3.0527\n",
      " -3.3474\n",
      " -2.8460\n",
      " -3.1386\n",
      " -3.6287\n",
      " -4.5408\n",
      " -3.0957\n",
      " -4.0851\n",
      " -3.3152\n",
      " -2.4600\n",
      " -2.9645\n",
      " -4.5151\n",
      " -4.5746\n",
      " -3.4726\n",
      " -4.5593\n",
      " -3.3245\n",
      " -3.1369\n",
      " -3.3995\n",
      " -2.4461\n",
      " -3.8722\n",
      " -4.3555\n",
      " -3.8254\n",
      " -3.1415\n",
      " -2.9362\n",
      " -3.7233\n",
      " -4.0243\n",
      " -2.9243\n",
      " -2.8619\n",
      " -3.5726\n",
      " -3.9910\n",
      " -2.4665\n",
      " -4.2769\n",
      " -2.9079\n",
      " -4.3945\n",
      " -2.9859\n",
      " -3.2609\n",
      " -2.3723\n",
      " -2.3630\n",
      " -2.9682\n",
      " -4.4386\n",
      " -3.4101\n",
      " -3.3163\n",
      " -3.5349\n",
      " -4.3824\n",
      " -4.0719\n",
      " -3.1012\n",
      " -3.0599\n",
      " -3.6653\n",
      " -4.2445\n",
      " -4.3558\n",
      " -4.1658\n",
      " -3.9407\n",
      " -3.9695\n",
      " -3.2121\n",
      " -2.7221\n",
      " -2.6603\n",
      " -2.4604\n",
      " -3.9905\n",
      " -3.2935\n",
      " -2.6825\n",
      " -2.6862\n",
      " -3.5300\n",
      " -3.6695\n",
      " -4.4906\n",
      " -4.4887\n",
      " -4.4626\n",
      " -4.3302\n",
      " -4.4280\n",
      " -4.1686\n",
      " -4.3202\n",
      " -4.3825\n",
      " -4.0637\n",
      " -3.6924\n",
      " -2.4553\n",
      " -3.9396\n",
      " -2.5687\n",
      " -3.7727\n",
      " -2.5404\n",
      " -4.1608\n",
      " -2.3751\n",
      " -4.3338\n",
      " -2.7413\n",
      " -2.6627\n",
      " -2.9234\n",
      " -4.2285\n",
      " -3.7045\n",
      " -2.7236\n",
      " -2.3321\n",
      " -4.4030\n",
      " -3.7305\n",
      " -2.4770\n",
      " -3.9976\n",
      " -4.0892\n",
      " -4.2611\n",
      " -2.6598\n",
      " -2.6656\n",
      " -4.3103\n",
      " -3.2025\n",
      " -4.0722\n",
      " -2.6713\n",
      " -2.3677\n",
      " -2.3658\n",
      " -4.5291\n",
      " -3.7080\n",
      " -3.0335\n",
      " -3.4144\n",
      " -2.9600\n",
      " -4.2555\n",
      " -2.8539\n",
      " -3.9908\n",
      " -4.3463\n",
      " -3.0707\n",
      " -2.3694\n",
      " -3.1320\n",
      " -4.0036\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-437867.16234375001, 87567.005410964106]) \n",
      "2600 L1: G:Variable containing:\n",
      "-6.1083e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 6.1083e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+06 *\n",
      " -4.7515\n",
      " -3.3879\n",
      " -5.6518\n",
      " -3.7323\n",
      " -3.6067\n",
      " -3.0517\n",
      " -4.6598\n",
      " -3.6788\n",
      " -3.5181\n",
      " -4.2294\n",
      " -4.9942\n",
      " -4.4580\n",
      " -3.3503\n",
      " -3.4976\n",
      " -5.0500\n",
      " -2.9002\n",
      " -3.7251\n",
      " -5.2398\n",
      " -5.6248\n",
      " -3.3259\n",
      " -3.8705\n",
      " -4.0087\n",
      " -5.6090\n",
      " -2.8728\n",
      " -3.6708\n",
      " -5.0158\n",
      " -4.8540\n",
      " -4.6919\n",
      " -4.2623\n",
      " -4.3410\n",
      " -5.1851\n",
      " -3.5655\n",
      " -4.9943\n",
      " -4.9681\n",
      " -4.4390\n",
      " -5.4481\n",
      " -2.9961\n",
      " -5.5117\n",
      " -4.2107\n",
      " -4.5741\n",
      " -4.6561\n",
      " -2.9885\n",
      " -4.2877\n",
      " -3.9025\n",
      " -5.5568\n",
      " -3.8015\n",
      " -4.2778\n",
      " -3.6614\n",
      " -4.1300\n",
      " -5.0295\n",
      " -3.6976\n",
      " -3.8367\n",
      " -4.6845\n",
      " -4.6098\n",
      " -5.3969\n",
      " -5.2440\n",
      " -4.5155\n",
      " -4.0118\n",
      " -3.0283\n",
      " -4.8440\n",
      " -5.2233\n",
      " -4.9477\n",
      " -3.1637\n",
      " -3.4860\n",
      " -4.1013\n",
      " -3.3874\n",
      " -4.6612\n",
      " -4.5796\n",
      " -4.9728\n",
      " -5.5499\n",
      " -4.8593\n",
      " -3.6538\n",
      " -3.2025\n",
      " -3.2665\n",
      " -3.9573\n",
      " -3.6785\n",
      " -5.3041\n",
      " -4.1759\n",
      " -4.1970\n",
      " -5.5726\n",
      " -3.0859\n",
      " -3.8467\n",
      " -5.6594\n",
      " -4.1788\n",
      " -4.4608\n",
      " -5.0476\n",
      " -4.7086\n",
      " -3.9799\n",
      " -4.5725\n",
      " -5.1398\n",
      " -3.1251\n",
      " -3.7741\n",
      " -3.9099\n",
      " -4.3729\n",
      " -4.4607\n",
      " -4.8126\n",
      " -3.5980\n",
      " -3.7736\n",
      " -4.5949\n",
      " -3.0383\n",
      " -4.4513\n",
      " -3.2693\n",
      " -5.4502\n",
      " -3.2269\n",
      " -3.2441\n",
      " -4.3353\n",
      " -3.8462\n",
      " -5.3887\n",
      " -4.7215\n",
      " -5.3518\n",
      " -5.0219\n",
      " -3.3479\n",
      " -3.5897\n",
      " -3.9193\n",
      " -5.5582\n",
      " -5.2789\n",
      " -3.1401\n",
      " -3.5186\n",
      " -5.0949\n",
      " -3.9583\n",
      " -3.1785\n",
      " -4.8377\n",
      " -4.0323\n",
      " -4.1118\n",
      " -3.6189\n",
      " -5.4208\n",
      " -5.5997\n",
      " -3.7282\n",
      " -5.5716\n",
      " -5.5434\n",
      " -5.4260\n",
      " -5.6318\n",
      " -4.8175\n",
      " -4.5953\n",
      " -4.1124\n",
      " -4.0899\n",
      " -4.4675\n",
      " -5.1737\n",
      " -4.2324\n",
      " -2.9551\n",
      " -5.6035\n",
      " -4.1919\n",
      " -4.5636\n",
      " -3.1022\n",
      " -3.6789\n",
      " -3.1475\n",
      " -4.6331\n",
      " -5.2596\n",
      " -4.3300\n",
      " -4.1387\n",
      " -4.9898\n",
      " -3.6089\n",
      " -2.9094\n",
      " -4.0205\n",
      " -5.2511\n",
      " -3.6305\n",
      " -2.9863\n",
      " -4.1531\n",
      " -4.1030\n",
      " -4.6054\n",
      " -4.8198\n",
      " -4.8331\n",
      " -4.2337\n",
      " -4.4931\n",
      " -3.6707\n",
      " -4.5881\n",
      " -5.0091\n",
      " -4.4913\n",
      " -3.6490\n",
      " -3.8859\n",
      " -4.4274\n",
      " -4.1376\n",
      " -3.7121\n",
      " -3.4909\n",
      " -3.8729\n",
      " -4.5973\n",
      " -5.2239\n",
      " -3.7068\n",
      " -4.4764\n",
      " -3.4295\n",
      " -2.9263\n",
      " -4.8929\n",
      " -4.8560\n",
      " -4.9740\n",
      " -4.8025\n",
      " -5.0307\n",
      " -5.6724\n",
      " -4.4828\n",
      " -3.7018\n",
      " -4.6382\n",
      " -3.3018\n",
      " -5.5476\n",
      " -3.8169\n",
      " -2.9215\n",
      " -4.6032\n",
      " -5.4868\n",
      " -4.0897\n",
      " -5.3348\n",
      " -3.8733\n",
      " -4.5188\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-543940.89593750006, 97686.623405357648]) \n",
      "2800 L1: G:Variable containing:\n",
      "-8.9163e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " M:Variable containing:\n",
      " 8.9163e+13\n",
      "[torch.FloatTensor of size 1]\n",
      " \n",
      "Variable containing:\n",
      "1.00000e+06 *\n",
      " -3.5190\n",
      " -6.2539\n",
      " -4.1376\n",
      " -4.9154\n",
      " -5.8648\n",
      " -6.3676\n",
      " -4.7411\n",
      " -4.7866\n",
      " -5.6740\n",
      " -3.5399\n",
      " -6.6837\n",
      " -5.3475\n",
      " -4.8855\n",
      " -6.1848\n",
      " -5.0865\n",
      " -5.6379\n",
      " -3.6225\n",
      " -4.2198\n",
      " -4.1629\n",
      " -4.7814\n",
      " -3.8480\n",
      " -6.7640\n",
      " -5.6339\n",
      " -4.5689\n",
      " -4.8853\n",
      " -5.2896\n",
      " -6.1074\n",
      " -5.0333\n",
      " -4.0766\n",
      " -5.4706\n",
      " -5.5265\n",
      " -3.8535\n",
      " -5.6482\n",
      " -6.3051\n",
      " -5.6333\n",
      " -3.4356\n",
      " -5.7198\n",
      " -6.3515\n",
      " -4.3620\n",
      " -5.4329\n",
      " -5.1449\n",
      " -3.9849\n",
      " -3.4442\n",
      " -3.4482\n",
      " -4.0820\n",
      " -4.8968\n",
      " -6.4841\n",
      " -6.0992\n",
      " -5.5000\n",
      " -3.5768\n",
      " -5.6255\n",
      " -4.8094\n",
      " -4.5690\n",
      " -5.6074\n",
      " -4.8209\n",
      " -5.3364\n",
      " -3.8160\n",
      " -5.9601\n",
      " -5.8303\n",
      " -3.5138\n",
      " -6.7212\n",
      " -4.6119\n",
      " -6.5711\n",
      " -6.7417\n",
      " -4.7942\n",
      " -3.8515\n",
      " -6.7307\n",
      " -6.2234\n",
      " -4.6575\n",
      " -6.7765\n",
      " -6.4387\n",
      " -6.1284\n",
      " -5.2699\n",
      " -4.8566\n",
      " -3.5906\n",
      " -6.1682\n",
      " -6.8016\n",
      " -6.1311\n",
      " -6.3011\n",
      " -4.8050\n",
      " -4.3468\n",
      " -5.3063\n",
      " -3.9180\n",
      " -5.0731\n",
      " -3.9830\n",
      " -6.3251\n",
      " -6.7612\n",
      " -6.3210\n",
      " -4.7110\n",
      " -4.9281\n",
      " -6.6104\n",
      " -5.2533\n",
      " -4.6667\n",
      " -6.3760\n",
      " -4.6107\n",
      " -6.3830\n",
      " -5.0097\n",
      " -5.6359\n",
      " -5.6128\n",
      " -6.2211\n",
      " -5.0558\n",
      " -5.8056\n",
      " -5.6168\n",
      " -5.9397\n",
      " -3.8765\n",
      " -5.1699\n",
      " -6.7591\n",
      " -4.6909\n",
      " -6.0212\n",
      " -3.4788\n",
      " -5.8645\n",
      " -5.9307\n",
      " -4.7968\n",
      " -5.9429\n",
      " -3.6899\n",
      " -6.2991\n",
      " -6.5356\n",
      " -4.5138\n",
      " -4.9339\n",
      " -4.3309\n",
      " -4.3922\n",
      " -4.9868\n",
      " -5.0775\n",
      " -5.6413\n",
      " -4.7800\n",
      " -6.2931\n",
      " -6.6893\n",
      " -4.9209\n",
      " -4.2564\n",
      " -6.6047\n",
      " -5.8309\n",
      " -4.6674\n",
      " -3.6492\n",
      " -4.5536\n",
      " -6.1800\n",
      " -5.5504\n",
      " -3.6349\n",
      " -6.3548\n",
      " -3.7048\n",
      " -3.7635\n",
      " -6.7520\n",
      " -3.7924\n",
      " -4.5972\n",
      " -4.3450\n",
      " -4.4155\n",
      " -4.0787\n",
      " -6.2352\n",
      " -4.9706\n",
      " -4.0295\n",
      " -6.4407\n",
      " -6.1752\n",
      " -4.3291\n",
      " -5.1476\n",
      " -6.2528\n",
      " -3.5623\n",
      " -6.1194\n",
      " -5.3448\n",
      " -3.9225\n",
      " -5.3042\n",
      " -4.7219\n",
      " -5.8908\n",
      " -5.3088\n",
      " -4.6490\n",
      " -6.5875\n",
      " -6.7113\n",
      " -5.2616\n",
      " -5.0642\n",
      " -3.6384\n",
      " -5.4835\n",
      " -6.0242\n",
      " -6.4636\n",
      " -3.5245\n",
      " -5.7186\n",
      " -3.6000\n",
      " -5.0132\n",
      " -6.8170\n",
      " -4.0734\n",
      " -5.0572\n",
      " -3.4259\n",
      " -5.8296\n",
      " -5.8402\n",
      " -4.0694\n",
      " -6.7073\n",
      " -5.6794\n",
      " -6.5743\n",
      " -4.7090\n",
      " -4.4351\n",
      " -4.0955\n",
      " -4.5071\n",
      " -6.3667\n",
      " -3.6313\n",
      " -5.4757\n",
      " -5.8363\n",
      " -5.0935\n",
      " -6.0805\n",
      " -6.1935\n",
      " -5.8418\n",
      " -3.8160\n",
      " -4.9775\n",
      " -4.6911\n",
      "[torch.FloatTensor of size 200x1]\n",
      " \n",
      " mu, std (Real: [0.0, 0.0], Fake: [-655958.51000000001, 124641.38190755734]) \n"
     ]
    }
   ],
   "source": [
    "exact_sampler = exact_model(data_mean, data_stddev)\n",
    "gi_sampler = get_generator_input_sampler()\n",
    "\n",
    "G = Generator(input_size=g_input_size, hidden_size=g_hidden_size, output_size=g_output_size)\n",
    "M = Model(input_size=m_input_size, hidden_size=m_hidden_size, output_size=m_output_size)\n",
    "\n",
    "g_criterion = nl1  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "consta = Variable(torch.Tensor([-1]).double())\n",
    "m_criterion = l1  # Binary cross entropy: http://pytorch.org/docs/nn.html#bceloss\n",
    "\n",
    "g_optimizer = optim.Adam(G.parameters(), lr=g_learning_rate, betas=optim_betas)\n",
    "m_optimizer = optim.Adam(M.parameters(), lr=m_learning_rate, betas=optim_betas)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    for g_index in range(g_steps):\n",
    "        # 2. Train G on D's response (but DO NOT train D on these labels)\n",
    "        G.zero_grad()\n",
    "        M.zero_grad()\n",
    "        \n",
    "        gen_input = Variable(gi_sampler(minibatch_size, g_input_size))\n",
    "        \n",
    "        states = G(gen_input)\n",
    "        fake_data = M(states)\n",
    "        true_data = exact_sampler(states)\n",
    "        \n",
    "        g_vars = G.parameters()\n",
    "        g_error = g_criterion(fake_data, true_data)  # we want to fool, so pretend it's all genuine\n",
    "        g_error.backward(retain_graph=True)\n",
    "        g_optimizer.step()  # Only optimizes G's parameters\n",
    "\n",
    "        m_vars = M.parameters()\n",
    "        m_error = m_criterion(fake_data, true_data)  # we want to fool, so pretend it's all genuine\n",
    "        m_error.backward()\n",
    "        m_optimizer.step()  # Only optimizes G's parameters\n",
    "\n",
    "    if epoch % print_interval == 0:\n",
    "        print(\"%s L1: G:%s M:%s \\n%s \\n mu, std (Real: %s, Fake: %s) \" % (epoch,\n",
    "                                                            g_error,\n",
    "                                                            m_error,\n",
    "                                                            states,\n",
    "                                                            stats(extract(true_data)),\n",
    "                                                            stats(extract(fake_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = G(Variable(torch.FloatTensor(np.array(np.linspace(-30, 30, num=500))).view(-1,1)))\n",
    "y = G(x)\n",
    "plt.plot(y.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(M(Variable(torch.Tensor(np.linspace(-3,10,1000)).view(-1,1).float())).data.numpy())\n",
    "plt.plot(exact_sampler(Variable(torch.Tensor(np.linspace(-3,10,1000)).view(-1,1).float())).data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
