{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'StyleData'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-082d36cd2dfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mConstant\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mload_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStyleData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mModelDefine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDsModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'StyleData'"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from Constant import Constants\n",
    "from load_data import StyleData\n",
    "from torch.autograd import Variable\n",
    "from ModelDefine import DsModel\n",
    "from ModelDefine import Embed\n",
    "from PreTrainDs import indexData2variable\n",
    "import random\n",
    "\n",
    "import time\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"  \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\" # change 0  with whatever card is available\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "# os.environ[\"CHAINER_DEBUG\"]=\"1\"\n",
    "torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "# there are four models need to be defined Ez, Ey, D, Ds(pre-trained)\n",
    "torch.cuda.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firstly we just do consistent encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DsModel(nn.Module):\n",
    "    \"\"\"\n",
    "    notes:\n",
    "        This model can also be called classfier\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kind_filters, num_filters, num_in_channels, embedded_size, hidden_size=128):\n",
    "        \"\"\"\n",
    "        Argus:\n",
    "        kind_filters is a list\n",
    "        num_filters is the number of filters we want use\n",
    "        num_in_channels in this case is the number of kinds of embedding\n",
    "        embedded_size is the embedding size (easy)\n",
    "        hidden_size = is the hidden_units' number we want to use\n",
    "        \n",
    "        Notice:\n",
    "        kind_filters need to be a list.\n",
    "        for instance, [1, 2, 3] represent the there are three kind of\n",
    "        window which's size is 1 or 2 or 3\n",
    "        the Ds have multi-filter-size and muti-convs-maps\n",
    "        \"\"\"\n",
    "        super(DsModel, self).__init__()\n",
    "\n",
    "        self.kind_filters = kind_filters\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        self.convs= nn.ModuleList([nn.Conv2d(num_in_channels, num_filters, (width, embedded_size)) for width in self.kind_filters])\n",
    "\n",
    "\n",
    "        # self.convs = nn.ModuleList([])\n",
    "        # for width in self.kind_filters:\n",
    "        #     self.convs.append(nn.Conv2d(num_in_channels, num_filters, (width, embedded_size)))\n",
    "\n",
    "        self.linear = nn.Linear(num_filters * len(kind_filters), hidden_size)\n",
    "        self.linear_out = nn.Linear(hidden_size, 2)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim = -1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        this model's inputs should like this N_batch*C_channel*Seq_length*Embedded_size\n",
    "        if we just use one kind embedding (dynamic or static) the C_channel is 1\n",
    "        if we use two kind of embedding (dynamic and static) the C_channel is 2\n",
    "        \n",
    "        the outputs is the probability of x1 < X1\n",
    "        \"\"\"\n",
    "        # convs_outputs = []\n",
    "        # for convs in self.convs:\n",
    "        #     convs_outputs.append(convs(x))\n",
    "\n",
    "        convs_outputs= [convs(x) for convs in self.convs]\n",
    "\n",
    "\n",
    "        max_pools_outputs = []\n",
    "        for outputs in convs_outputs:\n",
    "            max_pools_outputs.append(F.max_pool2d(outputs, kernel_size=(outputs.size()[2], 1)))\n",
    "            # [2] is the size of high\n",
    "        flatten = torch.cat(max_pools_outputs, dim=1).view(x.size()[0], -1)\n",
    "        return self.softmax(self.relu(self.linear_out(self.drop(self.relu(self.linear(flatten))))))\n",
    "\n",
    "\n",
    "class EzModel(nn.Module):\n",
    "    \"\"\"\n",
    "    this model take embedding as the input\n",
    "    this model is the decode model and it's hidden_output will be delivered to G model\n",
    "    this model is implemented with a GRU(RNN) and the last hidden outputs as\n",
    "    the encoded contents of input_sequence\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size, hidden_size):\n",
    "        super(EzModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(embedding_size, hidden_size).cuda()\n",
    "        # the GRU's output is special, 'hidden_out' of every time step excatly\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        x should look like this shape seq_len * batch * input_size\n",
    "        and as usual the batch is 1\n",
    "        the output is the all hidden and the hidden is the last hidden\n",
    "        \"\"\"\n",
    "        outputs, hidden = self.gru(x, hidden)\n",
    "        return self.relu(outputs), self.relu(hidden)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.hidden_size).cuda())  # the minibatch is 1\n",
    "\n",
    "\n",
    "class EyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    this model is the style decode model who's output is deliverd to G model\n",
    "    and this model's structure is also very similar with the Ds model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, num_filters, kind_filters, embedding_size):\n",
    "        super(EyModel, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "        self.kind_filters = kind_filters\n",
    "        self.num_filters = num_filters\n",
    "        self.y2_style = self.init_style()\n",
    "        self.x2_style_flag = 1\n",
    "        \n",
    "        self.convs= nn.ModuleList([nn.Conv2d(in_channels, num_filters, (width, embedding_size)) for width in self.kind_filters])\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, index):\n",
    "        \"\"\"\n",
    "        this model's input should like this N_batch*C_channel*Len_seqence*Width_{embedding_size}\n",
    "        and the input also should include the domain of x represent as {1,0} ,1 represent the \n",
    "        target domain\n",
    "        and his output is regard as the syle represent (size: 1 * (Kinds_filters*Num_filters))\n",
    "        \"\"\"\n",
    "        if index == self.x2_style_flag:\n",
    "            return self.y2_style\n",
    "        convs_outputs = []\n",
    "        # torch.cuda.synchronize()\n",
    "        for convs in self.convs:\n",
    "            y = convs(x)\n",
    "            convs_outputs.append(y)\n",
    "            \n",
    "        max_pools_outputs= [F.max_pool2d(outputs, kernel_size=(outputs.size()[2], 1)).view((-1,)) for outputs in convs_outputs]      \n",
    "        y1_style = torch.cat(max_pools_outputs).view(x.size()[0], -1)\n",
    "        return self.relu(y1_style)\n",
    "\n",
    "    def init_style(self):\n",
    "        return nn.Parameter(torch.randn(1, len(self.kind_filters) * self.num_filters), requires_grad=True)\n",
    "\n",
    "\n",
    "class GModel(nn.Module):\n",
    "    def __init__(self, hidden_size, n_vocab, embedding_size, temper):  # temper is the temperature\n",
    "        \"\"\"\n",
    "        hidden_size = z.size + y.size\n",
    "        embedding_size = 250\n",
    "        \"\"\"\n",
    "        super(GModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.temper = temper\n",
    "\n",
    "        self.gru = nn.GRU(embedding_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, embedding_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        hidden = cat(z.size + y.size)\n",
    "        x = prev_x1_hat\n",
    "        \"\"\"\n",
    "\n",
    "        hidden, _ = self.gru(x, hidden)\n",
    "        out_embedding = self.out(self.relu(hidden)).view(hidden.size()[0], -1)\n",
    "#         hidden = self.relu(hidden)\n",
    "#         x_hat, x_hat_noT, hidden\n",
    "        return self.softmax(out_embedding / self.temper), out_embedding, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(1, 1, self.hidden_size).cuda(), requires_grad=False)\n",
    "\n",
    "\n",
    "class DModel(nn.Module):\n",
    "    \"\"\"\n",
    "    DModel is also very like Ey & Ds and also have a sigmoid function as the output layer \n",
    "    and this model take G's hidden state and G's hidden state' length is dynamic\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kind_filters, num_filters, num_in_channels, width, hidden_size=128):\n",
    "\n",
    "        super(DModel, self).__init__()\n",
    "        self.kind_filters = kind_filters\n",
    "        self.num_filters = num_filters\n",
    "\n",
    "        # self.convs = nn.ModuleList([])\n",
    "        # for w in self.kind_filters:\n",
    "        #     self.convs.append(nn.Conv2d(num_in_channels, num_filters, (w, width)))\n",
    "\n",
    "        self.convs= nn.ModuleList([nn.Conv2d(num_in_channels, num_filters, (w, width)) for w in self.kind_filters]).cuda()\n",
    "\n",
    "\n",
    "        self.linear = nn.Linear(num_filters * len(kind_filters), hidden_size)\n",
    "        self.linear_out = nn.Linear(hidden_size, 2)\n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        the input is just the like this N_batch*C_channel*Seq_len*Width\n",
    "        and the C_channel is 1 because the GRU only output one hidden state every time t\n",
    "        \"\"\"\n",
    "        convs_outputs = []\n",
    "        for convs in self.convs:\n",
    "            convs_outputs.append(convs(x))\n",
    "\n",
    "        max_pools_outputs = []\n",
    "        for outputs in convs_outputs:\n",
    "            max_pools_outputs.append(F.max_pool2d(outputs, kernel_size=(outputs.size()[2], 1)))\n",
    "\n",
    "        flatten = torch.cat(max_pools_outputs, dim=1).view(x.size()[0], -1)\n",
    "        return self.softmax(self.relu(self.linear_out(self.relu(self.drop(self.linear(flatten))))))\n",
    "\n",
    "\n",
    "class Embed(nn.Module):\n",
    "    \"\"\"\n",
    "    this is the embedding layer which could embed the index and one-hot logit vector\n",
    "    but you should indicator use_one_hot or not with index = {True, False}\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_vocab, embedding_size):\n",
    "        super(Embed, self).__init__()\n",
    "        self.embedding = nn.Embedding(n_vocab, embedding_size)\n",
    "\n",
    "    def forward(self, x, index=True):\n",
    "        if index:\n",
    "            return self.embedding(x)\n",
    "        else:\n",
    "            return torch.mm(x, self.embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModel(nn.Module):\n",
    "    \"\"\"\n",
    "    this model is the gan model wich will reutrn many data we need to compute the loss so we just need\n",
    "    create one model called GAN\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, style_represent, content_represent, D_filters, D_num_filters, Ey_filters,\n",
    "                 Ey_num_filters, embedding_size, n_vocab, temper, max_len=40, min_len = 6, style_path= './data/style.npy'):\n",
    "        \"\"\"\n",
    "        style_represent is the dim we choose to represent the style\n",
    "        content_represent is the dim we choose to represent the content\n",
    "        D_filters is a list like this [1,2,3,4]\n",
    "        D_num_filters is the the filters number we want to use for each window size\n",
    "        Ey_filters\n",
    "        \"\"\"\n",
    "        super(GANModel, self).__init__()\n",
    "        self.style_represent = Ey_num_filters * len(Ey_filters)\n",
    "        self.temper = temper\n",
    "        self.max_len = max_len\n",
    "        self.min_len = min_len\n",
    "        self.style_data = np.load(style_path)\n",
    "\n",
    "        self.Ez = EzModel(embedding_size, content_represent).cuda()  # hidden_size is the content_represent\n",
    "        # content_represent == conten_represent\n",
    "        self.Ey = EyModel(1, Ey_num_filters, Ey_filters, embedding_size).cuda()\n",
    "        # style_represent == Ey_num_filters * Len(Ey_filters)\n",
    "        self.G = GModel(content_represent + self.style_represent, n_vocab, embedding_size, temper).cuda()\n",
    "#         self.D = DModel(D_filters, D_num_filters, 1, content_represent + self.style_represent).cuda()\n",
    "        \n",
    "        \n",
    "        self.embedding = Embed(n_vocab, embedding_size)\n",
    "        self.embedding = self.embedding.cuda()\n",
    "        self.go = self.embedding(Variable(torch.LongTensor([0]).cuda()))\n",
    "        self.go = self.go.cuda()\n",
    "\n",
    "    def forward(self, x1, Ez_train=True,\n",
    "                G_train=True,\n",
    "                D_train=True,\n",
    "                Embedd_train=True,\n",
    "                Ey_train=True,\n",
    "                Lcyc=True,\n",
    "                Ladv=True,\n",
    "                Ldis=True,\n",
    "                Lrec=True):\n",
    "        \"\"\"\n",
    "        the input x don't need to have a <go>, but must have an <EOS>\n",
    "        The input x1, x2's shape should look like this (Len_seq)\n",
    "        addition the N_batch must equal to 1 if we want to add batch training we can consider \n",
    "        implement outside the model because we need to use many middle output to compute the loss it will be \n",
    "        very complicated if we compute inside the model\n",
    "        Notice:\n",
    "        there is something we need to pay attention to is will the y_start will be changed, if \n",
    "        y_start is not be changed we need to consider a method to update the y_star\n",
    "        \"\"\"\n",
    "        self.Ez.train(Ez_train)\n",
    "        self.Ey.train(Ey_train)\n",
    "#         self.D.train(D_train)\n",
    "        self.G.train(G_train)\n",
    "        self.embedding.train(Embedd_train)\n",
    "\n",
    "        # x1 and x2 is index represent\n",
    "        embedd_x1 = self.embedding(x1)\n",
    "        \n",
    "        y1 = self.Ey(embedd_x1.unsqueeze(0).unsqueeze(0), 0)  # we need to shape the 2d variable to 4d variable\n",
    "\n",
    "        hidden = self.Ez.init_hidden()\n",
    "\n",
    "\n",
    "        outputs, z1 = self.Ez(embedd_x1.unsqueeze(1).cuda(), hidden)\n",
    "        \n",
    "        x1_seq_len = x1.size()[0] #n_symbols\n",
    "    \n",
    "        x1_hat, x1_hat_noT, x1_hat_hid = self.get_x_hat_hidden(z1, y1, x1_seq_len)\n",
    "\n",
    "        return {'x1': embedd_x1,\n",
    "                'x1_hat_noT': x1_hat_noT,\n",
    "                'x1_hat_hid': x1_hat_hid,\n",
    "                'x1_hat': x1_hat}\n",
    "\n",
    "    # get y and z as input output the same len as x\n",
    "    def get_x_hat_hidden(self, z, y, seq_len, length_fix=True):\n",
    "        x_hats = []\n",
    "        x_hats_noT = []\n",
    "        hiddens = []\n",
    "        embedd_x_hat = []\n",
    "        self.go = self.go.cuda()\n",
    "        x_hat, x_hat_noT, hidden = self.G(self.go.view(1, 1, -1),\n",
    "                                          torch.cat([z.view(1, -1), y], dim=-1).view(1, 1, -1))\n",
    "        x_hats.append(x_hat)\n",
    "        x_hats_noT.append(x_hat_noT)\n",
    "        hiddens.append(hidden)\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "\n",
    "#             embedd_x_hat = self.embedding(x_hat, index=False)\n",
    "            x_hat, x_hat_noT, hidden = self.G(x_hat.view(1, 1, -1), hidden.view(1, 1, -1))\n",
    "\n",
    "            # the sequence's length be generated should be larger than 6 at least \n",
    "            if x_hat.topk(1)[1].data.cpu().numpy() == 1 and not length_fix and i >= self.min_len:\n",
    "                break\n",
    "#             embedd_x_hats.append(embedd_x_hat)\n",
    "            x_hats.append(x_hat)\n",
    "            x_hats_noT.append(x_hat_noT)\n",
    "            hiddens.append(hidden)\n",
    "\n",
    "        return torch.cat(x_hats), torch.cat(x_hats_noT), torch.cat(hiddens)  # cat in the first dim\n",
    "\n",
    "    def ind_to_words(self, ind_sent):\n",
    "    \treturn (' '.join([self.style_data[1][x] for x in ind_sent]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainVAE_D(epoches,batch_size,data,ds_model,ds_emb,gan_path,style_path,pretrainD=False):\n",
    "    style_data = np.load(style_path)\n",
    "\n",
    "    gan = torch.load(gan_path)\n",
    "    # gan.apply(weights_init) # apply weight init\n",
    "\n",
    "        #     style_represent is the dim we choose to represent the style\n",
    "        # content_represent is the dim we choose to represent the content\n",
    "        # D_filters is a list like this [1,2,3,4]\n",
    "        # D_num_filters is the the filters number we want to use for each window size\n",
    "        # Ey_filters\n",
    "    # gan = GANModel(style_represent=500, content_represent=250, D_filters=[2,3,4,5,6], D_num_filters=100, Ey_filters=[1,2,3,4,5],\n",
    "    #              Ey_num_filters=100, embedding_size=250, n_vocab=8981, temper=0.0001, max_len=40, min_len = 6, style_path=style_path)\n",
    "    gan = gan.cuda()\n",
    "    gan.train(True)\n",
    "    style = StyleData()\n",
    "    style.load(style_path)\n",
    "    const = Constants(style.n_words)\n",
    "    optimizer = optim.Adam(gan.parameters(),lr=1e-3)\n",
    "    lamda1 = 1\n",
    "    lamda2 = 1\n",
    "    lamda3 = 3\n",
    "    cross_entropy = nn.CrossEntropyLoss()\n",
    "#     emb_loss = \n",
    "    # init the state of some model\n",
    "    ds_model.train(True)\n",
    "    ds_emb.train(True)\n",
    "\n",
    "    \n",
    "    train_data = indexData2variable(data)\n",
    "    train_data = build2pairs(train_data)\n",
    "    \n",
    "    for i in range(epoches):\n",
    "        print((\"epoches:\\t\", i))\n",
    "        if pretrainD:\n",
    "            print(\"trainning Discriminator..........\")\n",
    "        else :\n",
    "            print(\"trainning Generator..............\")\n",
    "        sys.stdout.flush()\n",
    "        stime = time.time()\n",
    "        \n",
    "        shuffleData(train_data)\n",
    "        print(len(train_data))\n",
    "        sys.stdout.flush()\n",
    "        count = 0\n",
    "        # for count in range(int(len(train_data))):\n",
    "        while count < int(len(train_data)-batch_size):\n",
    "            tempdata = train_data[count:count+batch_size]\n",
    "            \n",
    "            if tempdata == []:\n",
    "                break\n",
    "                \n",
    "            count += batch_size\n",
    "            optimizer.zero_grad()\n",
    "            loss = 0\n",
    "\n",
    "            for seqs in tempdata:\n",
    "                seqs[0] = seqs[0].cuda()\n",
    "                dic = gan(seqs[0],D_train=True)\n",
    "#                 loss = emb_loss(, seqs[0])\n",
    "                loss = (1-F.cosine_similarity(dic['x1_hat_noT'], dic['x1'])).mean()\n",
    "    \n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "            if count%4000 == 0:\n",
    "                print('l:{}, {} / {}'.format(loss, count,len(train_data)))\n",
    "                W = gan.embedding.embedding.weight\n",
    "                W_normalized = W.div(W.norm(p = 2, dim=1, keepdim=True))\n",
    "#                 print(unembed(dic['x1'], W_normalized, style_data)[1])\n",
    "                print(unembed(dic['x1_hat_noT'], W_normalized, style_data)[1])\n",
    "                print(ind_to_words(seqs[0].detach().cpu().numpy(), style_data))\n",
    "#                 print(onehot_to_words(dic['x1_hat_noT'], style_data))\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            \n",
    "        torch.save(gan, MODEL_NAME)\n",
    "        \n",
    "#         if acc > 0.8:\n",
    "#             pretrainD = False\n",
    "#         if acc < 0.6:\n",
    "#             pretrainD = True\n",
    "            \n",
    "            \n",
    "            \n",
    "        etime = time.time()\n",
    "        print((\"cost time \\t%.2f mins\" % ((etime - stime)/60)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    torch.save(gan, MODEL_NAME)\n",
    "            \n",
    "                \n",
    "def build2pairs(train_data):\n",
    "    data = []\n",
    "    for i in range(min( len(train_data[0]), len(train_data[1]) )):\n",
    "           data.append([train_data[0][i], train_data[1][i]])\n",
    "    return data\n",
    "\n",
    "def shuffleData(train_data):\n",
    "    \"\"\"\n",
    "    this function don't need to return any value and the list is changed inplace\n",
    "    \"\"\"\n",
    "    if len(train_data) == 2:\n",
    "        random.shuffle(train_data[0])\n",
    "        random.shuffle(train_data[1])\n",
    "    else:\n",
    "        random.shuffle(train_data)\n",
    "        \n",
    "def unembed(x1, W_normalized, style_data):\n",
    "    #calculate cosine similarity\n",
    "    x1_normalized = x1.div(x1.norm(p = 2, dim=1, keepdim=True))\n",
    "    emb_distances = torch.mm(x1_normalized, W_normalized.t())\n",
    "    token_ids = torch.argmax(emb_distances, dim=1).cpu().numpy()\n",
    "    sentense = ' '.join([style_data[1][x] for x in token_ids])\n",
    "    return (token_ids, sentense)\n",
    "\n",
    "def ind_to_words(ind_sent, style_data):\n",
    "    return (' '.join([style_data[1][x] for x in ind_sent]))\n",
    "\n",
    "def onehot_to_words(onehot, style_data):\n",
    "    ind_sent = onehot.argmax(dim=1).cpu().numpy()\n",
    "    print(onehot)\n",
    "    print(ind_sent)\n",
    "    return (' '.join([style_data[1][x] for x in ind_sent]))\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = './Model/my_bithugan2.pkl'\n",
    "style = StyleData()\n",
    "style.load('./data/style.npy')\n",
    "const = Constants(n_vocab=style.n_words)\n",
    "print('content_represent', const.Content_represent)\n",
    "print('D_filters', const.D_filters)\n",
    "print('D_num_filters', const.Ds_num_filters)\n",
    "print('embedding_size', const.Embedding_size)\n",
    "print('Ey_filters', const.Ey_filters)\n",
    "print('Ey_num_filters', const.Ey_num_filters)\n",
    "print('n_vocab', const.N_vocab)\n",
    "print('style_represent', const.Style_represent)\n",
    "print('temper', const.Temper)\n",
    "\n",
    "# gan = GANModel(content_represent=const.Content_represent,\n",
    "#                D_filters=const.D_filters,\n",
    "#                D_num_filters=const.Ds_num_filters,\n",
    "#                embedding_size=const.Embedding_size,\n",
    "#                Ey_filters=const.Ey_filters,\n",
    "#                Ey_num_filters=const.Ey_num_filters,\n",
    "#                n_vocab=const.N_vocab,\n",
    "#                style_represent=const.Style_represent,\n",
    "#                temper=const.Temper)  # there are 9 parameters of a GAN\n",
    "# torch.save(gan, MODEL_NAME)\n",
    "print('finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ds = torch.load('./Model/Ds_pretrained.pkl').cuda()\n",
    "ds_emb = torch.load('./Model/Ds_emb_pretrained.pkl').cuda()\n",
    "\n",
    "train_data = np.load('./data/trainDataOfIndex.npy')\n",
    "gan_path = MODEL_NAME\n",
    "style_path = './data/style.npy'\n",
    "epoches = 30\n",
    "batch_size = 50\n",
    "pretrainD = False\n",
    "\n",
    "trainVAE_D(epoches, batch_size, train_data, ds, ds_emb,  gan_path, style_path,pretrainD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from load_data import load_data, StyleDataset, Numerator\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_list = [\n",
    "    'data/sentiment.train.0',\n",
    "    'data/sentiment.train.1'\n",
    "#     'data/trump',\n",
    "#     'data/musk'\n",
    "]\n",
    "\n",
    "X, y, numerator = load_data(dataset_list, fasttext_location = '/mnt/wiki.simple.bin')\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=420)\n",
    "\n",
    "train_loader = DataLoader(StyleDataset(X_train, y_train, numerator.embeddings, sentence_size=15),\n",
    "                          batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x,y in train_loader:\n",
    "    print(x.size(), y.size())\n",
    "    print(' '.join([numerator.unembed(x[0,i].cpu().numpy()) for i in range(15)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
